{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBn7YkjpZiFY"
      },
      "source": [
        "# Main Classification\n",
        "\n",
        "## 3 feature tiers:\n",
        "- Model 1: Baseline (Crunchbase static only)\n",
        "- Model 2: Macro (includes funding (timing) and Macro Econ)\n",
        "- Model 3: Full (all features, w/ NLP)\n",
        "\n",
        "## Classifiers:\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- XGBoost\n",
        "- CatBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4noYsM45dZDR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDd6q16edeKl"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3o_N176D7MWU",
        "outputId": "daf0b3ba-c894-4218-b7ab-76a86ce14a03"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnxAN6DXdXWx",
        "outputId": "18b5faac-c455-407f-ae73-cd30ece9877a"
      },
      "outputs": [],
      "source": [
        "# !pip install shap --no-binary shap xgboost scikit-learn\n",
        "\n",
        "# !pip install shap lightgbm catboost joblib scikit-learn xgboost pandas numpy matplotlib tqdm --upgrade\n",
        "# Restart runtime after installing!\n",
        "\n",
        "import os\n",
        "import shap\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time # For timing blocks\n",
        "\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split # Use RandomizedSearchCV\n",
        "from sklearn.metrics import (\n",
        "   roc_auc_score, accuracy_score,\n",
        "   precision_score, recall_score, f1_score, make_scorer\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.svm import SVC # Commented out SVM/MLP for brevity, can be added back\n",
        "# from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure display options are set for pandas DataFrames\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "from google.colab import drive, runtime\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhXyCybudfIL"
      },
      "source": [
        "### Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ1WL0Mtdg0m",
        "outputId": "ac7dd2af-46ac-4455-8ade-e6ce2149908c"
      },
      "outputs": [],
      "source": [
        "input_folder = '/content/drive/MyDrive/Senior/Thesis/Code/Data/Input Data/'\n",
        "model_data_folder = input_folder + 'model_data'\n",
        "output_folder = '/content/drive/MyDrive/Senior/Thesis/Code/Data/Output Data/Model'\n",
        "\n",
        "# Path for metric checkpoint files (unique per run potentially)\n",
        "run_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(f\"Run timestamp: {run_timestamp}\")\n",
        "model_results_checkpoints_path = os.path.join(output_folder, f\"checkpoint_model_metrics_{run_timestamp}.csv\")\n",
        "\n",
        "# Check contents of folders\n",
        "model_data_contents = os.listdir(model_data_folder)\n",
        "print(model_data_contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiNsGM_TD13b"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMdePu0LD3LZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(model_data_folder, 'merged_startup_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BNyOO7exFX6B",
        "outputId": "1e2105ed-8da0-4207-a8ed-c9abf2f75f72"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(df.info())\n",
        "print()\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cNiq1AAXYlNz",
        "outputId": "2abe61cc-6af5-4007-a7ce-1f879f73982b"
      },
      "outputs": [],
      "source": [
        "null_summary = df.isnull().sum().sort_values(ascending=False)\n",
        "print(null_summary.head(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIcv6PcDbsyS"
      },
      "source": [
        "## Define Feature Set + Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "554wgYss3l8t"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uxXnofR0Zaw",
        "outputId": "97f28d19-bc92-45a0-9c72-e14869db9e1c"
      },
      "outputs": [],
      "source": [
        "# ============================ CONFIG ============================\n",
        "\n",
        "# --- Modeling Choices ---\n",
        "USE_ITERATIVE_IMPUTER = False  # Set to True to use IterativeImputer (slower), False for SimpleImputer+indicator\n",
        "CV_FOLDS = 3                   # Number of cross-validation folds for hyperparameter tuning\n",
        "N_ITER_RANDOM_SEARCH = 30      # Number of parameter combinations to try in RandomizedSearchCV (adjust based on time)\n",
        "FILTER_YEAR = True             # Set to False to include startups founded after 2020 (use with caution for long-term labels)\n",
        "FILTER_YEAR_THRESHOLD = 2020\n",
        "\n",
        "# --- Target Labels to Run ---\n",
        "# TARGET_COLS_TO_RUN = [\"success_label_5y\", \"success_label_10y\"]\n",
        "TARGET_COLS_TO_RUN = [\"success_label_5y\"]\n",
        "\n",
        "# Optional: Filter by founding year based on config\n",
        "if FILTER_YEAR:\n",
        "    initial_rows = df.shape[0]\n",
        "    df = df[df[\"founded_year\"] <= FILTER_YEAR_THRESHOLD].copy()\n",
        "    print(f\"Filtered DataFrame to founded_year <= {FILTER_YEAR_THRESHOLD}. Shape: {df.shape} (removed {initial_rows - df.shape[0]} rows)\")\n",
        "\n",
        "# ============================ CLASSIFIERS ============================\n",
        "\n",
        "# Define base classifiers with relevant default settings\n",
        "# Note: random_state is set for reproducibility\n",
        "# Note: class_weight/scale_pos_weight handled in train_evaluate_model\n",
        "classifiers = {\n",
        "    # \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1),\n",
        "    # \"LightGBM\": LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1), # Suppress verbosity\n",
        "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0, thread_count=-1), # Use all cores, suppress verbose output\n",
        "    # \"RandomForest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    # \"LogisticRegression\": LogisticRegression(max_iter=2000, random_state=42, solver='saga', n_jobs=-1) # Saga supports L1/L2 and is often good for larger datasets\n",
        "}\n",
        "\n",
        "\n",
        "# Define WIDER parameter grids for RandomizedSearchCV\n",
        "# Using lists here, but consider scipy.stats distributions for a broader random search\n",
        "param_grids_random = {\n",
        "    # \"XGBoost\": {\n",
        "    #     'n_estimators': [100, 200, 300, 400, 500],\n",
        "    #     'max_depth': [3, 5, 7, 9, 11],\n",
        "    #     'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "    #     'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    #     'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    #     'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
        "    #     'reg_alpha': [0, 0.01, 0.1, 1],      # L1\n",
        "    #     'reg_lambda': [0.5, 1, 1.5, 2, 3]       # L2\n",
        "    # },\n",
        "    #  \"LightGBM\": {\n",
        "    #     'n_estimators': [100, 200, 300, 500, 700],\n",
        "    #     'max_depth': [-1, 5, 10, 15, 20, 25], # -1 means no limit\n",
        "    #     'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
        "    #     'num_leaves': [31, 63, 127, 200, 255], # Typically < 2^max_depth\n",
        "    #     'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    #     'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    #     'reg_alpha': [0, 0.01, 0.1, 0.5, 1], # L1\n",
        "    #     'reg_lambda': [0, 0.01, 0.1, 0.5, 1]  # L2\n",
        "    # },\n",
        "    \"CatBoost\": {\n",
        "        'iterations': [100, 200, 300, 500, 700],\n",
        "        'depth': [4, 6, 8, 10],\n",
        "        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
        "        'l2_leaf_reg': [1, 3, 5, 7, 9], # L2 regularization\n",
        "        'border_count': [32, 64, 128, 255], # Controls discretization\n",
        "        'subsample': [0.7, 0.8, 0.9, 1.0] # Row sampling\n",
        "    },\n",
        "    # \"RandomForest\": {\n",
        "    #     \"n_estimators\": [100, 200, 300, 400, 500],\n",
        "    #     \"max_depth\": [10, 20, 30, 40, None], # Explore deeper trees\n",
        "    #     \"min_samples_split\": [2, 5, 10, 15],\n",
        "    #     \"min_samples_leaf\": [1, 3, 5, 7],\n",
        "    #     \"max_features\": ['sqrt', 'log2'],\n",
        "    #     \"class_weight\": [\"balanced\", \"balanced_subsample\"] # Try both balancing strategies\n",
        "    # },\n",
        "    # \"LogisticRegression\": {\n",
        "    #     \"C\": [0.01, 0.1, 1, 10, 50, 100],\n",
        "    #     \"penalty\": [\"l1\", \"l2\"]\n",
        "    #     # solver='saga', class_weight='balanced' set in constructor\n",
        "    # }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58CA7dwS3hm8"
      },
      "source": [
        "### Feature Engineering and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yQQ1v1BDOoN",
        "outputId": "1821f09c-0e8b-4125-ac59-907c1d75b6cd"
      },
      "outputs": [],
      "source": [
        "# ============================ FEATURE ENGINEERING & PREPROCESSING ============================\n",
        "\n",
        "# ---- Binarize Industry ----\n",
        "print(\"Binarizing 'industry' column...\")\n",
        "# Ensure clean parsing of industry column\n",
        "df[\"industry_split\"] = df[\"industry\"].fillna(\"\").apply(lambda x: [s.strip() for s in x.split(\",\")])\n",
        "\n",
        "\n",
        "# Define the 9 known industries\n",
        "known_industries = [\n",
        "   \"Life Sciences\", \"Fintech\", \"Consumer Goods\", \"Technology\",\n",
        "   \"Cleantech\", \"Transportation\", \"Media Entertainment and Gaming\",\n",
        "   \"Telecom\", \"Real Estate\"\n",
        "]\n",
        "\n",
        "# Binarize them using MultiLabelBinarizer with fixed class order\n",
        "mlb = MultiLabelBinarizer(classes=known_industries)\n",
        "industry_df = pd.DataFrame(\n",
        "   mlb.fit_transform(df[\"industry_split\"]),\n",
        "   columns=[f\"industry_{c.replace(' ', '_').replace('_and_', '_')}\" for c in mlb.classes_] # Cleaner names\n",
        ")\n",
        "\n",
        "# Concatenate binary columns and drop the original/temp\n",
        "df = pd.concat([df.drop(columns=[\"industry\", \"industry_split\"]), industry_df], axis=1)\n",
        "industry_columns = industry_df.columns.tolist()\n",
        "print(\"Industry binarization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db0wCk_hZag3",
        "outputId": "9ee1bbd9-ce18-410c-ec32-1f38eeba11ca"
      },
      "outputs": [],
      "source": [
        "# ---- STEP 1: Define Feature Sets ----\n",
        "\n",
        "# STATIC_MODEL — Basic Crunchbase attributes only\n",
        "features_static = [\n",
        "    'founded_year', 'city', 'founder_count',\n",
        "    'founder_gender_diversity', 'has_top_school_founder',\n",
        "    'num_optimal_degrees', 'optimal_degree_ratio',\n",
        "    'is_repeat_founder', 'founder_gender_missing', 'founder_degree_missing',\n",
        "    'founder_school_missing', 'founder_desc_missing',\n",
        "    'has_funding_data', 'num_disclosed_rounds', 'has_disclosed_funding',\n",
        "    'first_funding_amount_bucket', 'is_startup_hub',\n",
        "    'cohort_funding_density', 'investor_count', 'has_known_investor'\n",
        "] + industry_columns\n",
        "\n",
        "# MACRO_MODEL — Add funding + macroeconomic/timing features\n",
        "features_macro = features_static + [\n",
        "    # Timing & funding\n",
        "    'age_at_first_funding', 'first_funding_delay', 'early_series_count',\n",
        "    'avg_time_to_early_round_months', 'avg_time_between_rounds',\n",
        "    'burn_rate', 'funding_velocity',\n",
        "\n",
        "    # Economic & market indicators\n",
        "    'gdp_growth_avg_15m', 'gdp_growth_delta_3m', 'interest_rate_fed_funds_avg_15m',\n",
        "    'interest_rate_fed_funds_delta_3m', 'fed_funds_rate_latest',\n",
        "    'yield_curve_10y_2y_avg_15m', 'yield_curve_inversion_flag',\n",
        "    'unemployment_rate_avg_15m', 'cpi_inflation_avg_15m',\n",
        "    'consumer_sentiment_avg_15m', 'consumer_sentiment_z_latest',\n",
        "    'vix_index_max_15m', 'vix_spike_flag', 'vix_latest',\n",
        "    'sp500_price_change_3m', 'sp500_volatility_3m', 'sp500_momentum_latest',\n",
        "    'avg_etf_return_3m', 'avg_etf_volatility_3m', 'avg_etf_momentum_latest',\n",
        "    'avg_etf_golden_cross_flag', 'avg_etf_ma50_to_price_ratio'\n",
        "]\n",
        "\n",
        "# FULL_MODEL — All above + LLM and sentiment features\n",
        "features_full = features_macro + [\n",
        "    'org_desc_sentiment_finbert', 'founder_desc_sentiment_finbert',\n",
        "    'org_desc_sim_exemplar', 'founder_desc_sim_exemplar',\n",
        "    'llm_founder_score', 'industry_outlook_sentiment_finbert',\n",
        "    'industry_timing_sentiment_finbert',\n",
        "    'llm_outlook_align_score_avg', 'llm_outlook_align_score_binned'\n",
        "]\n",
        "\n",
        "# ---- STEP 2: Store in dictionary for easier looped modeling ----\n",
        "model_feature_sets = {\n",
        "    \"STATIC_MODEL\": features_static,\n",
        "    # \"MACRO_MODEL\": features_macro,\n",
        "    \"FULL_MODEL\": features_full,\n",
        "}\n",
        "\n",
        "print(\"Feature sets defined.\")\n",
        "print(f\"STATIC features: {len(features_static)}\")\n",
        "# print(f\"MACRO features: {len(features_macro)}\")\n",
        "print(f\"FULL features: {len(features_full)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoDHtvHyILIH"
      },
      "source": [
        "## Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEF7yh_eIOEQ"
      },
      "outputs": [],
      "source": [
        "# ============================ DATA PREP FUNCTION ============================\n",
        "def prepare_data_for_model(df, feature_list, target_col, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Prepares data for a specific model tier and target.\n",
        "    Includes splitting, handling inf/all-NaN columns, imputation, scaling, and encoding.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        feature_list (list): List of feature names to use.\n",
        "        target_col (str): The name of the target column.\n",
        "        test_size (float): Proportion of data for the test set.\n",
        "        random_state (int): Random seed for reproducibility.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        tuple: X_train_transformed, X_test_transformed, y_train, y_test, fitted_preprocessor\n",
        "    \"\"\"\n",
        "    print(f\"Preparing data for target '{target_col}' with {len(feature_list)} features...\")\n",
        "    df_model = df.dropna(subset=[target_col])\n",
        "    X = df_model[feature_list].copy()\n",
        "    y = df_model[target_col]\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
        "    )\n",
        "    print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "\n",
        "    # --- Clean infinite values and drop columns that became all NaN after split ---\n",
        "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    X_train = X_train.dropna(axis=1, how=\"all\")\n",
        "    X_test = X_test[X_train.columns]  # Align test columns to train columns\n",
        "\n",
        "\n",
        "    # --- Identify Feature Types ---\n",
        "    categorical_features = list(X_train.select_dtypes(include=['object', 'category']).columns)\n",
        "    # Ensure 'city' and 'first_funding_amount_bucket' are treated as categorical if they exist\n",
        "    manual_cats = ['city', 'first_funding_amount_bucket']\n",
        "    for cat in manual_cats:\n",
        "        if cat in X_train.columns and cat not in categorical_features:\n",
        "             categorical_features.append(cat)\n",
        "\n",
        "    numerical_features = list(X_train.select_dtypes(include=np.number).columns)\n",
        "    # Remove any manual cats that might have been inferred as numeric (e.g., if encoded previously)\n",
        "    numerical_features = [f for f in numerical_features if f not in categorical_features]\n",
        "\n",
        "    print(f\"Identified {len(numerical_features)} numerical features.\")\n",
        "    print(f\"Identified {len(categorical_features)} categorical features.\")\n",
        "\n",
        "\n",
        "    # --- Define Preprocessing Steps ---\n",
        "    if USE_ITERATIVE_IMPUTER:\n",
        "        print(\"Using IterativeImputer for numerical features.\")\n",
        "        numeric_imputer = IterativeImputer(max_iter=10, random_state=random_state) # Faster iteration count\n",
        "    else:\n",
        "        print(\"Using SimpleImputer (median) with missing indicators for numerical features.\")\n",
        "        # Add indicator for SimpleImputer is crucial for tree models with high missingness\n",
        "        numeric_imputer = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
        "\n",
        "\n",
        "    # Using most_frequent for categorical is standard\n",
        "    categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "    numeric_transformer = Pipeline([\n",
        "        (\"imputer\", numeric_imputer),\n",
        "        (\"scaler\", StandardScaler()) # Scale after imputation\n",
        "    ])\n",
        "    categorical_transformer = Pipeline([\n",
        "        (\"imputer\", categorical_imputer),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # Use sparse=False for easier SHAP handling later if needed\n",
        "    ])\n",
        "\n",
        "    # --- Create ColumnTransformer ---\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, numerical_features),\n",
        "            (\"cat\", categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough' # Keep any columns not specified (shouldn't be any here)\n",
        "    )\n",
        "\n",
        "\n",
        "    # --- Fit and Transform ---\n",
        "    print(\"Fitting preprocessor and transforming data...\")\n",
        "    preprocessor.fit(X_train)\n",
        "    X_train_transformed = preprocessor.transform(X_train)\n",
        "    X_test_transformed = preprocessor.transform(X_test)\n",
        "    print(\"Data transformation complete.\")\n",
        "\n",
        "    # --- Get Feature Names After Transformation ---\n",
        "    # This is essential for interpreting SHAP plots correctly\n",
        "    try:\n",
        "        feature_names_out = preprocessor.get_feature_names_out()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not automatically get feature names. Error: {e}. Manual construction might be needed.\")\n",
        "        feature_names_out = None # Handle this case downstream\n",
        "\n",
        "\n",
        "    return X_train_transformed, X_test_transformed, y_train.astype(int), y_test.astype(int), preprocessor, feature_names_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgKLLIvYHzx6"
      },
      "source": [
        "## Training, Eval, SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-7aLX3K4L7L"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtTtYzAvHzmG"
      },
      "outputs": [],
      "source": [
        "# ============================ MODEL TRAINING & EVALUATION ============================\n",
        "\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test, model_name, classifier_config, param_grid_config, cv_folds, n_iter_search, random_state=42):\n",
        "    \"\"\"\n",
        "    Trains a model using RandomizedSearchCV (if grid provided), handles class imbalance, and evaluates.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        X_train, X_test, y_train, y_test: Data splits.\n",
        "        model_name (str): Name of the model (key in classifiers).\n",
        "        classifier_config (dict): Dictionary of base classifier instances.\n",
        "        param_grid_config (dict): Dictionary of parameter grids for RandomizedSearchCV.\n",
        "        cv_folds (int): Number of CV folds for tuning.\n",
        "        n_iter_search (int): Number of iterations for RandomizedSearchCV.\n",
        "        random_state (int): Random seed.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        tuple: best_model, auc, acc, prec, rec, f1, best_params\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    print(f\"  Training {model_name}...\")\n",
        "    model = classifier_config[model_name] # Get a fresh instance\n",
        "\n",
        "\n",
        "    # --- Handle Class Imbalance ---\n",
        "    scale_pos_weight = None\n",
        "    # Calculate only if model supports it and isn't already balanced by class_weight\n",
        "    if model_name in [\"XGBoost\", \"CatBoost\", \"LightGBM\"] and 'class_weight' not in model.get_params():\n",
        "        neg, pos = np.bincount(y_train)\n",
        "        if pos > 0: scale_pos_weight = neg / pos\n",
        "        print(f\"Calculated scale_pos_weight for {model_name}: {scale_pos_weight:.2f}\")\n",
        "\n",
        "\n",
        "    # Apply scale_pos_weight or class_weight to the base model *before* tuning\n",
        "    try:\n",
        "        if scale_pos_weight is not None:\n",
        "             if model_name == \"XGBoost\": model.set_params(scale_pos_weight=scale_pos_weight)\n",
        "             if model_name == \"CatBoost\": model.set_params(scale_pos_weight=scale_pos_weight)\n",
        "             if model_name == \"LightGBM\": model.set_params(scale_pos_weight=scale_pos_weight) # or is_unbalance=True / class_weight='balanced'\n",
        "        # class_weight='balanced'/'balanced_subsample' already set in classifier definition for RF/LR\n",
        "    except Exception as e:\n",
        "        print(f\"    Warning: Could not set imbalance parameter for {model_name}. Error: {e}\")\n",
        "\n",
        "\n",
        "    # --- Hyperparameter Tuning ---\n",
        "    if model_name in param_grid_config:\n",
        "        print(f\"Running RandomizedSearchCV with n_iter={n_iter_search}, cv={cv_folds}...\")\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=param_grid_config[model_name],\n",
        "            n_iter=n_iter_search,\n",
        "            cv=cv_folds,\n",
        "            scoring='roc_auc', # Optimize for AUC during tuning\n",
        "            verbose=0,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1 # Use all cores\n",
        "        )\n",
        "        random_search.fit(X_train, y_train)\n",
        "        best_model = random_search.best_estimator_\n",
        "        best_params = random_search.best_params_\n",
        "        print(f\"Best params found: {best_params}\")\n",
        "    else:\n",
        "        # Fit default model if no grid (no need for logisitic regression - taking too long)\n",
        "        print(\"No param grid defined, fitting default model.\")\n",
        "        best_model = model.fit(X_train, y_train)\n",
        "        best_params = model.get_params()\n",
        "\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0) # Handle zero division\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Training completed in {(end_time - start_time):.2f} seconds. AUC: {auc:.4f}, Acc: {acc:.4f}\")\n",
        "\n",
        "    return best_model, auc, acc, prec, rec, f1, best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q02UNPWHy4YE"
      },
      "outputs": [],
      "source": [
        "# ============================ SHAP ANALYSIS ============================\n",
        "\n",
        "def get_ranked_shap_features(model, X_train_transformed, feature_names):\n",
        "    \"\"\"\n",
        "    Calculates SHAP values and returns a DataFrame of features ranked by mean absolute SHAP value.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model: The trained model instance.\n",
        "        X_train_transformed: The preprocessed training data used to train the model.\n",
        "        feature_names (list): List of feature names corresponding to X_train_transformed columns.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'feature' and 'mean_abs_shap', sorted descending.\n",
        "                      Returns None if SHAP calculation fails.\n",
        "    \"\"\"\n",
        "    print(\"Calculating SHAP values for feature importance...\")\n",
        "    start_time = time.time()\n",
        "    shap_values_for_importance = None\n",
        "    explainer = None\n",
        "\n",
        "    try:\n",
        "        # Choose appropriate explainer\n",
        "        if isinstance(model, (XGBClassifier, LGBMClassifier, CatBoostClassifier, RandomForestClassifier)):\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values_calc = explainer.shap_values(X_train_transformed)\n",
        "            # Handle potential list output for binary classification in some SHAP/model versions\n",
        "            if isinstance(shap_values_calc, list) and len(shap_values_calc) == 2:\n",
        "                shap_values_for_importance = shap_values_calc[1] # SHAP values for the positive class\n",
        "            else:\n",
        "                shap_values_for_importance = shap_values_calc # Assume it returns values for positive class\n",
        "\n",
        "        elif isinstance(model, LogisticRegression):\n",
        "            # LinearExplainer is efficient for linear models\n",
        "            explainer = shap.LinearExplainer(model, X_train_transformed)\n",
        "            shap_values_for_importance = explainer.shap_values(X_train_transformed)\n",
        "\n",
        "        else: # Fallback for SVM, MLP etc. - can be slow!\n",
        "            print(\"Warning: Using KernelExplainer for importance. Subsampling data...\")\n",
        "            # Sample background data for KernelExplainer efficiency\n",
        "            X_train_summary = shap.sample(X_train_transformed, 100 if X_train_transformed.shape[0] > 100 else X_train_transformed.shape[0])\n",
        "            explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
        "            # Calculate SHAP for a subset of X_train for feasibility\n",
        "            shap_values_list = explainer.shap_values(X_train_transformed[:500], nsamples='auto') # Explain first 500 samples\n",
        "            if isinstance(shap_values_list, list) and len(shap_values_list) == 2:\n",
        "                shap_values_for_importance = shap_values_list[1] # SHAP values for class 1\n",
        "            else:\n",
        "                 shap_values_for_importance = shap_values_list\n",
        "\n",
        "        if shap_values_for_importance is None:\n",
        "             print(\"Error: Could not obtain SHAP values.\")\n",
        "             return None\n",
        "\n",
        "        # Ensure correct shape (samples, features)\n",
        "        if len(shap_values_for_importance.shape) > 2:\n",
        "            shap_values_for_importance = shap_values_for_importance[:, :, 1]\n",
        "\n",
        "        # Calculate mean absolute SHAP value\n",
        "        mean_abs_shap = np.abs(shap_values_for_importance).mean(axis=0)\n",
        "\n",
        "        # Create DataFrame\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'mean_abs_shap': mean_abs_shap\n",
        "        })\n",
        "\n",
        "        # Sort by importance\n",
        "        feature_importance_df = feature_importance_df.sort_values(by='mean_abs_shap', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"  SHAP value calculation completed in {(end_time - start_time):.2f} seconds.\")\n",
        "        return feature_importance_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  SHAP calculation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- SHAP Plotting Function (Optional - can be integrated or separate) ---\n",
        "def plot_shap_summary(model, X_train_transformed, feature_names, title):\n",
        "    \"\"\"Generates and shows a SHAP beeswarm plot.\"\"\"\n",
        "    print(f\"\\nGenerating SHAP Summary Plot for {title}\")\n",
        "    try:\n",
        "         # Re-calculate SHAP specifically for plotting if needed, or use values if passed\n",
        "         explainer = None\n",
        "         if isinstance(model, (XGBClassifier, LGBMClassifier, CatBoostClassifier, RandomForestClassifier)):\n",
        "             explainer = shap.TreeExplainer(model)\n",
        "             shap_values_plot = explainer(X_train_transformed) # Use explainer.__call__ for Explanation object\n",
        "         elif isinstance(model, LogisticRegression):\n",
        "              explainer = shap.LinearExplainer(model, X_train_transformed)\n",
        "              shap_values_plot = explainer(X_train_transformed)\n",
        "         else:\n",
        "             print(f\"Warning: Using KernelExplainer for plot {title}. This can be slow.\")\n",
        "             X_train_summary = shap.sample(X_train_transformed, 100)\n",
        "             explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
        "             shap_values_list = explainer.shap_values(X_train_transformed[:200], nsamples='auto') # Explain fewer for plot\n",
        "             # Need to construct Explanation object manually for KernelExplainer plot\n",
        "             shap_values_plot = shap.Explanation(values=shap_values_list[1], # Values for positive class\n",
        "                                                  base_values=explainer.expected_value[1],\n",
        "                                                  data=X_train_transformed[:200],\n",
        "                                                  feature_names=feature_names)\n",
        "\n",
        "         shap.plots.beeswarm(shap_values_plot, max_display=30, show=False)\n",
        "         plt.title(title)\n",
        "         plt.tight_layout()\n",
        "         plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "         print(f\"SHAP plot generation failed for {title}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s-mrSg7zFkm"
      },
      "outputs": [],
      "source": [
        "# ============================ RETRAINING PIPELINE ============================\n",
        "def retrain_with_top_features(df, target_col, original_results_df, feature_importance_df, k_features, model_tier_to_retrain=\"FULL_MODEL\", random_state=42):\n",
        "    \"\"\"\n",
        "    Retrains the best model from a specific tier using only the top k features identified by SHAP.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The original DataFrame (before splitting/preprocessing).\n",
        "        target_col (str): Name of the target variable column.\n",
        "        original_results_df (pd.DataFrame): DataFrame containing results from the full_pipeline run.\n",
        "        feature_importance_df (pd.DataFrame): Sorted DataFrame from get_ranked_shap_features.\n",
        "        k_features (int): Number of top features to use for retraining.\n",
        "        model_tier_to_retrain (str): The feature tier whose best model we are retraining.\n",
        "        random_state (int): Random seed.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing metrics and info for the retrained model, or None if fails.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== RETRAINING WITH TOP {k_features} FEATURES for target '{target_col}' ===\")\n",
        "\n",
        "\n",
        "    if feature_importance_df is None or k_features > feature_importance_df.shape[0]:\n",
        "        print(f\"  Error: Invalid feature importance data or k ({k_features}) > total features.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # --- Identify Best Original Model and its Params ---\n",
        "    tier_results = original_results_df[original_results_df['Feature_Tier'] == model_tier_to_retrain]\n",
        "    if tier_results.empty:\n",
        "        print(f\"  Error: No results found for tier '{model_tier_to_retrain}' in original results.\")\n",
        "        return None\n",
        "\n",
        "    best_original_row = tier_results.sort_values(by=\"AUC\", ascending=False).iloc[0]\n",
        "    best_model_name = best_original_row['Model']\n",
        "    best_original_params = best_original_row['Best_Params']\n",
        "    original_preprocessor = best_original_row['Preprocessor'] # Get the original preprocessor\n",
        "    print(f\"  Identified best original model: {best_model_name}\")\n",
        "    print(f\"  Using best hyperparameters found previously: {best_original_params}\")\n",
        "\n",
        "\n",
        "    # --- Select Top K Features ---\n",
        "    # Important: We need the *original* feature names before one-hot encoding etc.\n",
        "    # This requires careful handling based on how get_feature_names_out formats them.\n",
        "    # We assume the preprocessor stores the original mapping or we derive it.\n",
        "    # For now, we select based on the *transformed* feature names from SHAP.\n",
        "    # A more robust solution might involve mapping back to originals if complex interactions aren't used.\n",
        "    top_k_feature_names_transformed = feature_importance_df['feature'].head(k_features).tolist()\n",
        "    print(f\"  Selected top {k_features} transformed features based on SHAP importance.\")\n",
        "\n",
        "\n",
        "    # --- Prepare Data with ONLY Selected Transformed Features ---\n",
        "    # This is tricky. We need to apply the *original* preprocessor and then select columns.\n",
        "    # Alternative: Modify the original feature list *before* prepare_data_for_model.\n",
        "    # Let's try modifying the feature list first, as it's cleaner if SHAP names map back easily.\n",
        "\n",
        "\n",
        "    # **Strategy 1: Modify Original Feature List (Simpler if mapping is clear)**\n",
        "    # This requires mapping SHAP features (e.g., 'num__feature', 'cat__city_NY') back to\n",
        "    # original features ('feature', 'city'). This is non-trivial.\n",
        "\n",
        "\n",
        "    # **Strategy 2: Filter AFTER Transformation (More Robust)**\n",
        "    # We need the full data prepared first, then filter the transformed arrays.\n",
        "    original_feature_list = model_feature_sets[model_tier_to_retrain]\n",
        "    X_train_orig_tf, X_test_orig_tf, y_train, y_test, _, feature_names_out_orig = prepare_data_for_model(\n",
        "        df, original_feature_list, target_col, random_state=random_state\n",
        "    )\n",
        "\n",
        "\n",
        "    # Find the indices corresponding to the top k transformed features\n",
        "    try:\n",
        "        top_k_indices = [list(feature_names_out_orig).index(f) for f in top_k_feature_names_transformed]\n",
        "    except ValueError as e:\n",
        "        print(f\"  Error: Could not find all top k features in the preprocessed feature names. {e}\")\n",
        "        # This can happen if feature names changed slightly or SHAP picked an unexpected one.\n",
        "        # Maybe try intersection:\n",
        "        top_k_feature_names_transformed = list(set(top_k_feature_names_transformed) & set(feature_names_out_orig))\n",
        "        top_k_indices = [list(feature_names_out_orig).index(f) for f in top_k_feature_names_transformed]\n",
        "        print(f\"  Using intersection: {len(top_k_indices)} features.\")\n",
        "        if not top_k_indices: return None\n",
        "\n",
        "\n",
        "    # Filter the transformed data arrays\n",
        "    X_train_selected_tf = X_train_orig_tf[:, top_k_indices]\n",
        "    X_test_selected_tf = X_test_orig_tf[:, top_k_indices]\n",
        "    print(f\"  Filtered transformed data to shape: Train {X_train_selected_tf.shape}, Test {X_test_selected_tf.shape}\")\n",
        "\n",
        "\n",
        "    # --- Retrain Best Model with Best Params on Selected Features ---\n",
        "    print(f\"Retraining {best_model_name} with selected features...\")\n",
        "    retrained_model = classifiers[best_model_name] # Get new instance\n",
        "    retrained_model.set_params(**best_original_params) # Apply best params\n",
        "\n",
        "\n",
        "    # Apply imbalance handling again (important!)\n",
        "    scale_pos_weight = None\n",
        "    if best_model_name in [\"XGBoost\", \"CatBoost\", \"LightGBM\"] and 'class_weight' not in best_original_params:\n",
        "        neg, pos = np.bincount(y_train)\n",
        "        if pos > 0: scale_pos_weight = neg / pos\n",
        "    try:\n",
        "        if scale_pos_weight is not None:\n",
        "             if best_model_name == \"XGBoost\": retrained_model.set_params(scale_pos_weight=scale_pos_weight)\n",
        "             if best_model_name == \"CatBoost\": retrained_model.set_params(scale_pos_weight=scale_pos_weight)\n",
        "             if best_model_name == \"LightGBM\": retrained_model.set_params(scale_pos_weight=scale_pos_weight)\n",
        "        elif 'class_weight' in best_original_params and best_original_params['class_weight'] == 'balanced':\n",
        "             # Ensure balanced is set if it was the best param for RF/LR\n",
        "             if best_model_name in [\"RandomForest\", \"LogisticRegression\"]:\n",
        "                 retrained_model.set_params(class_weight='balanced')\n",
        "\n",
        "    except Exception as e:\n",
        "         print(f\"    Warning: Could not set imbalance parameter during retraining. Error: {e}\")\n",
        "\n",
        "\n",
        "    retrained_model.fit(X_train_selected_tf, y_train)\n",
        "\n",
        "\n",
        "    # --- Evaluate Retrained Model ---\n",
        "    y_pred_retrained = retrained_model.predict(X_test_selected_tf)\n",
        "    y_proba_retrained = retrained_model.predict_proba(X_test_selected_tf)[:, 1]\n",
        "\n",
        "    auc_re = roc_auc_score(y_test, y_proba_retrained)\n",
        "    acc_re = accuracy_score(y_test, y_pred_retrained)\n",
        "    prec_re = precision_score(y_test, y_pred_retrained, zero_division=0)\n",
        "    rec_re = recall_score(y_test, y_pred_retrained, zero_division=0)\n",
        "    f1_re = f1_score(y_test, y_pred_retrained, zero_division=0)\n",
        "\n",
        "\n",
        "    print(f\"  Retraining complete. AUC: {auc_re:.4f}, Acc: {acc_re:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"Target\": target_col,\n",
        "        \"Retrained_Model\": best_model_name,\n",
        "        \"Num_Features\": k_features,\n",
        "        \"AUC\": auc_re,\n",
        "        \"Accuracy\": acc_re,\n",
        "        \"Precision\": prec_re,\n",
        "        \"Recall\": rec_re,\n",
        "        \"F1\": f1_re,\n",
        "        \"Features_Used_Names\": top_k_feature_names_transformed # Transformed names\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cmIGH0C0r3R"
      },
      "outputs": [],
      "source": [
        "# ============================ FULL PIPELINE ============================\n",
        "def run_full_experiment(df, target_cols, feature_sets_dict, classifier_dict, param_grid_dict, cv, n_iter, random_state):\n",
        "    \"\"\"\n",
        "    Runs the complete training, evaluation, and SHAP analysis pipeline for multiple targets and tiers.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe.\n",
        "        target_cols (list): List of target column names to iterate through.\n",
        "        feature_sets_dict (dict): Dictionary mapping tier names to feature lists.\n",
        "        classifier_dict (dict): Dictionary of base classifiers.\n",
        "        param_grid_dict (dict): Dictionary of parameter grids for tuning.\n",
        "        cv (int): Number of CV folds.\n",
        "        n_iter (int): Number of iterations for RandomizedSearchCV.\n",
        "        random_state (int): Random seed.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing all results.\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for target in target_cols:\n",
        "        print(f\"\\n{'='*20} RUNNING EXPERIMENT FOR TARGET: {target} {'='*20}\")\n",
        "        results_for_target = [] # Store results specific to this target run\n",
        "\n",
        "        # --- Run initial pipeline for all tiers ---\n",
        "        for tier_name, features in tqdm(feature_sets_dict.items(), desc=f\"Tier Progress ({target})\"):\n",
        "            print(f\"\\n=== Feature Tier: {tier_name} ({target}) ===\")\n",
        "            start_tier_time = time.time()\n",
        "            try:\n",
        "                X_train, X_test, y_train, y_test, preprocessor, feature_names_out = prepare_data_for_model(\n",
        "                    df, features, target, random_state=random_state\n",
        "                )\n",
        "            except Exception as e:\n",
        "                 print(f\"  Error during data preparation for tier {tier_name}: {e}. Skipping tier.\")\n",
        "                 continue # Skip to next tier if data prep fails\n",
        "\n",
        "            if feature_names_out is None:\n",
        "                 print(f\"  Skipping tier {tier_name} due to inability to get feature names.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            tier_results_current_run = [] # Temp list for models within this tier/target run\n",
        "\n",
        "            for model_name in tqdm(classifier_dict.keys(), desc=f\"{tier_name} Models ({target})\", leave=False):\n",
        "                print(f\"--- Training {model_name} on {tier_name} ({target})... ---\")\n",
        "                try:\n",
        "                    model, auc, acc, prec, rec, f1, best_params = train_evaluate_model(\n",
        "                        X_train, X_test, y_train, y_test, model_name,\n",
        "                        classifier_dict, param_grid_dict, cv, n_iter, random_state\n",
        "                    )\n",
        "\n",
        "                    # === Check model is fitted ===\n",
        "                    try:\n",
        "                        check_is_fitted(model)\n",
        "                        n_features = getattr(model, \"n_features_in_\", \"Unknown\")\n",
        "                        print(f\"Fitted {model_name}: {n_features} features\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Model {model_name} is not fitted: {e}. Skipping save.\")\n",
        "                        continue\n",
        "\n",
        "                    # === Save model and preprocessor ===\n",
        "                    model_filename = f\"best_model_{target}_{tier_name}_{model_name}_{run_timestamp}.pkl\"\n",
        "                    prep_filename = f\"preprocessor_{target}_{tier_name}_{model_name}_{run_timestamp}.pkl\"\n",
        "                    model_path = os.path.join(output_folder, model_filename)\n",
        "                    prep_path = os.path.join(output_folder, prep_filename)\n",
        "\n",
        "                    joblib.dump(model, model_path)\n",
        "                    joblib.dump(preprocessor, prep_path)\n",
        "                    print(f\"Saved model to {model_filename}\")\n",
        "                    print(f\"Saved preprocessor to {prep_filename}\")\n",
        "\n",
        "                    # ===========================\n",
        "\n",
        "                    result_row = {\n",
        "                        \"Target\": target, # Add target column identifier\n",
        "                        \"Feature_Tier\": tier_name,\n",
        "                        \"Model\": model_name,\n",
        "                        \"AUC\": auc,\n",
        "                        \"Accuracy\": acc,\n",
        "                        \"Precision\": prec,\n",
        "                        \"Recall\": rec,\n",
        "                        \"F1\": f1,\n",
        "                        \"Estimator\": model, # Store the fitted model object\n",
        "                        \"Preprocessor\": preprocessor, # Store the fitted preprocessor\n",
        "                        \"Feature_Names\": feature_names_out, # Store the output feature names\n",
        "                        \"X_train_Transformed\": X_train, # Store transformed data for SHAP\n",
        "                        \"Best_Params\": best_params\n",
        "                    }\n",
        "                    tier_results_current_run.append(result_row)\n",
        "\n",
        "                    # --- Checkpointing (Improved: include all results so far) ---\n",
        "                    temp_results_for_checkpoint = all_results + tier_results_current_run\n",
        "                    checkpoint_data = [{k: v for k, v in row.items() if k not in [\"Estimator\", \"X_train_Transformed\", \"Feature_Names\", \"Preprocessor\"]} for row in temp_results_for_checkpoint]\n",
        "                    checkpoint_df = pd.DataFrame(checkpoint_data)\n",
        "                    checkpoint_df.to_csv(model_results_checkpoints_path, index=False)\n",
        "                    print(f\"Checkpoint saved after {model_name} on {tier_name} ({target}).\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error training/evaluating {model_name} on {tier_name} ({target}): {e}\")\n",
        "\n",
        "\n",
        "            # --- Find best model for this tier/target and get SHAP ---\n",
        "            if tier_results_current_run:\n",
        "                 tier_results_df = pd.DataFrame(tier_results_current_run).sort_values(by=\"AUC\", ascending=False)\n",
        "                 top_row_tier = tier_results_df.iloc[0] # Best model for this tier/target\n",
        "\n",
        "                 print(f\"\\n--- SHAP Analysis for Best Model in Tier: {tier_name} ({top_row_tier['Model']}) ---\")\n",
        "                 feature_importance_df_tier = get_ranked_shap_features(\n",
        "                     model=top_row_tier[\"Estimator\"],\n",
        "                     X_train_transformed=top_row_tier[\"X_train_Transformed\"],\n",
        "                     feature_names=top_row_tier[\"Feature_Names\"]\n",
        "                 )\n",
        "                 # Optionally plot here too if desired, or just calculate importance\n",
        "                 # plot_shap_summary(top_row_tier[\"Estimator\"], top_row_tier[\"X_train_Transformed\"], top_row_tier[\"Feature_Names\"], f\"{tier_name} | {top_row_tier['Model']}\")\n",
        "\n",
        "                 # Store importance DF with the result row for later use (optional, can be large)\n",
        "                 # Note: This adds a DataFrame to each row, might be better to store separately\n",
        "                 # top_row_tier[\"SHAP_Importance_DF\"] = feature_importance_df_tier\n",
        "\n",
        "                 # Append results for this tier/target run to the main list for this target\n",
        "                 results_for_target.extend(tier_results_current_run)\n",
        "            else:\n",
        "                 print(f\"  No models successfully trained for tier {tier_name}, skipping SHAP.\")\n",
        "\n",
        "            end_tier_time = time.time()\n",
        "            print(f\"=== Tier {tier_name} ({target}) completed in {(end_tier_time - start_tier_time):.2f} seconds ===\")\n",
        "\n",
        "        # Append all results for this target to the overall results\n",
        "        all_results.extend(results_for_target)\n",
        "\n",
        "\n",
        "    # --- Final Results Processing ---\n",
        "    if not all_results:\n",
        "         print(\"\\nNo results were generated across all targets.\")\n",
        "         return pd.DataFrame()\n",
        "\n",
        "\n",
        "    results_df_final = pd.DataFrame(all_results)\n",
        "    # Sort by Target, Tier, then AUC\n",
        "    results_df_final = results_df_final.sort_values(by=[\"Target\", \"Feature_Tier\", \"AUC\"], ascending=[True, True, False])\n",
        "\n",
        "    print(\"\\nExperiment Run Complete.\")\n",
        "    return results_df_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmW4_DGa4Pxx"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "d14393c6899349c3b1b290ac9963624b",
            "679026781293439b9e7c2acd38a45f8a",
            "75c6fca14181452399898f2737c63b2a",
            "2343507107204f43a39c99cb68ca6dc0",
            "636be7695e544e71a5d74ee382883ba8",
            "5c49fae3467a49ad927c2ae88e97e0fb",
            "04f02547af544bcdbdcc7b8a3f5eb900",
            "9869b28705084a51be561ba311d3c4e7",
            "bb7704d281f7456d91f75493b5ef4c41",
            "a77383fdd36f4d85abe1420b5c3de4ca",
            "ed90c401a15247c4bd9973bb489ec430",
            "e8bbf27b1b4c4eb2abf33d922d918aa5",
            "b0260b0cccb044498d61d2bac8cad711",
            "3ae2692cc1e645da8e242248c3c06855",
            "348aa6d854454666858261544993f341",
            "96d6ede7b0594a13a0359425ad67b8fe",
            "1d026d49fb3a48d4b5cab585a8f0c9fd",
            "f4a58c464f1140d98d84227eda96c739",
            "2c3f08320e834d779e389667022ca0cd",
            "de8e1a38fc16457dbf839fe9878f3bea",
            "8533cede4b374ae8981dab6467cd8274",
            "fa22d2f18f42438cb73385cf8e5ed8b3",
            "cb19af75c0c84dae9883383041d84a3f"
          ]
        },
        "id": "LF9wY4lAzQZk",
        "outputId": "65f6a0b5-5df1-48d5-9277-898618cf6fcc"
      },
      "outputs": [],
      "source": [
        "# ============================ MAIN EXECUTION ============================\n",
        "\n",
        "# --- Run the main pipeline for selected targets ---\n",
        "all_run_results_df = run_full_experiment(\n",
        "    df=df,\n",
        "    target_cols=TARGET_COLS_TO_RUN,\n",
        "    feature_sets_dict=model_feature_sets,\n",
        "    classifier_dict=classifiers,\n",
        "    param_grid_dict=param_grids_random,\n",
        "    cv=CV_FOLDS,\n",
        "    n_iter=N_ITER_RANDOM_SEARCH,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# --- Display Top Results ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TOP MODEL RESULTS (METRICS ONLY)\")\n",
        "print(\"=\"*50)\n",
        "if not all_run_results_df.empty:\n",
        "    # Select columns to display (exclude objects)\n",
        "    display_cols = [\"Target\", \"Feature_Tier\", \"Model\", \"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Best_Params\"]\n",
        "    print(all_run_results_df[display_cols].head(20))\n",
        "\n",
        "    # Save final metrics summary\n",
        "    final_metrics_path = os.path.join(output_folder, f\"final_model_metrics_summary_{run_timestamp}.csv\")\n",
        "    all_run_results_df[display_cols].to_csv(final_metrics_path, index=False)\n",
        "    print(f\"\\nFinal metrics summary saved to: {final_metrics_path}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"No results to display.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTYzcfYrDA4a"
      },
      "source": [
        "### SHAP & Re-run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XSshLxElzTok"
      },
      "outputs": [],
      "source": [
        "# --- Optional: Get SHAP importance for Overall Best Model (e.g., best AUC on FULL_MODEL for 5y) ---\n",
        "if not all_run_results_df.empty:\n",
        "    target_for_shap = \"success_label_5y\" # Choose the target for detailed SHAP\n",
        "    tier_for_shap = \"FULL_MODEL\"       # Choose the tier\n",
        "    best_overall_row = all_run_results_df[\n",
        "        (all_run_results_df['Target'] == target_for_shap) &\n",
        "        (all_run_results_df['Feature_Tier'] == tier_for_shap)\n",
        "    ].sort_values(by=\"AUC\", ascending=False).iloc[0:1] # Use iloc[0:1] to keep as DataFrame\n",
        "\n",
        "\n",
        "    if not best_overall_row.empty:\n",
        "        best_overall_row = best_overall_row.iloc[0] # Now it's a Series\n",
        "        print(f\"\\n{'='*20} SHAP IMPORTANCE FOR OVERALL BEST ({target_for_shap}, {tier_for_shap}, {best_overall_row['Model']}) {'='*20}\")\n",
        "        shap_importance_df_best = get_ranked_shap_features(\n",
        "            model=best_overall_row[\"Estimator\"],\n",
        "            X_train_transformed=best_overall_row[\"X_train_Transformed\"],\n",
        "            feature_names=best_overall_row[\"Feature_Names\"]\n",
        "        )\n",
        "        if shap_importance_df_best is not None:\n",
        "            print(\"\\nTop 30 Most Important Features:\")\n",
        "            print(shap_importance_df_best.head(30))\n",
        "            importance_path_best = os.path.join(output_folder, f\"feature_importance_BEST_{best_overall_row['Model']}_{target_for_shap}_{tier_for_shap}.csv\")\n",
        "            shap_importance_df_best.to_csv(importance_path_best, index=False)\n",
        "            print(f\"\\nBest model's feature importance saved to: {importance_path_best}\")\n",
        "\n",
        "            # Optionally plot the best one again here\n",
        "            plot_shap_summary(best_overall_row[\"Estimator\"], best_overall_row[\"X_train_Transformed\"], best_overall_row[\"Feature_Names\"], f\"BEST: {tier_for_shap} | {best_overall_row['Model']} ({target_for_shap})\")\n",
        "\n",
        "            # --- Optional: Retrain with Top K Features ---\n",
        "            # K_FEATURES = 50 # Example: Retrain with top 50 features\n",
        "            # if shap_importance_df_best.shape[0] >= K_FEATURES:\n",
        "            #     retrain_results = retrain_with_top_features(\n",
        "            #         df=df, # Pass original df\n",
        "            #         target_col=target_for_shap,\n",
        "            #         original_results_df=all_run_results_df[all_run_results_df['Target'] == target_for_shap], # Pass results for the specific target\n",
        "            #         feature_importance_df=shap_importance_df_best,\n",
        "            #         k_features=K_FEATURES,\n",
        "            #         model_tier_to_retrain=tier_for_shap,\n",
        "            #         random_state=42\n",
        "            #     )\n",
        "            #     if retrain_results:\n",
        "            #         print(\"\\nRetraining with Top K Features Results:\")\n",
        "            #         print(pd.Series(retrain_results))\n",
        "            #         # Append retraining results to a separate file or list if needed\n",
        "            # else:\n",
        "            #     print(f\"\\nNot enough features ({shap_importance_df_best.shape[0]}) to select top {K_FEATURES}.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Could not find results for target '{target_for_shap}' and tier '{tier_for_shap}' to extract SHAP importance.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN88rtS1AK_o"
      },
      "outputs": [],
      "source": [
        "# --- Optional: Save Best Models ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING BEST MODEL PER TIER PER TARGET\")\n",
        "print(\"=\"*50)\n",
        "if not all_run_results_df.empty:\n",
        "    for target in all_run_results_df[\"Target\"].unique():\n",
        "         for tier in all_run_results_df[\"Feature_Tier\"].unique():\n",
        "             tier_target_results = all_run_results_df[\n",
        "                 (all_run_results_df['Target'] == target) &\n",
        "                 (all_run_results_df['Feature_Tier'] == tier)\n",
        "             ]\n",
        "             if not tier_target_results.empty:\n",
        "                 best_row = tier_target_results.sort_values(by=\"AUC\", ascending=False).iloc[0]\n",
        "                 model_filename = f\"best_model_{target}_{tier}_{best_row['Model']}_{run_timestamp}.pkl\"\n",
        "                 prep_filename = f\"preprocessor_{target}_{tier}_{run_timestamp}.pkl\"\n",
        "                 estimator_path = os.path.join(output_folder, model_filename)\n",
        "                 preprocessor_path = os.path.join(output_folder, prep_filename)\n",
        "\n",
        "\n",
        "                 joblib.dump(best_row[\"Estimator\"], estimator_path)\n",
        "                 joblib.dump(best_row[\"Preprocessor\"], preprocessor_path)\n",
        "                 print(f\"Saved: {model_filename} (AUC: {best_row['AUC']:.4f})\")\n",
        "                 print(f\"Saved: {prep_filename}\")\n",
        "             else:\n",
        "                 print(f\"No results found for Target: {target}, Tier: {tier}. Skipping save.\")\n",
        "else:\n",
        "     print(\"No results to save models from.\")\n",
        "\n",
        "print(\"\\nPipeline Finished.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "554wgYss3l8t",
        "58CA7dwS3hm8",
        "JoDHtvHyILIH",
        "o-7aLX3K4L7L",
        "MhLTF0ij4YkU"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04f02547af544bcdbdcc7b8a3f5eb900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d026d49fb3a48d4b5cab585a8f0c9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2343507107204f43a39c99cb68ca6dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a77383fdd36f4d85abe1420b5c3de4ca",
            "placeholder": "​",
            "style": "IPY_MODEL_ed90c401a15247c4bd9973bb489ec430",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "2c3f08320e834d779e389667022ca0cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348aa6d854454666858261544993f341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8533cede4b374ae8981dab6467cd8274",
            "placeholder": "​",
            "style": "IPY_MODEL_fa22d2f18f42438cb73385cf8e5ed8b3",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "3ae2692cc1e645da8e242248c3c06855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3f08320e834d779e389667022ca0cd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de8e1a38fc16457dbf839fe9878f3bea",
            "value": 0
          }
        },
        "5c49fae3467a49ad927c2ae88e97e0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636be7695e544e71a5d74ee382883ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "679026781293439b9e7c2acd38a45f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c49fae3467a49ad927c2ae88e97e0fb",
            "placeholder": "​",
            "style": "IPY_MODEL_04f02547af544bcdbdcc7b8a3f5eb900",
            "value": "Tier Progress (success_label_5y):   0%"
          }
        },
        "75c6fca14181452399898f2737c63b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9869b28705084a51be561ba311d3c4e7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb7704d281f7456d91f75493b5ef4c41",
            "value": 0
          }
        },
        "8533cede4b374ae8981dab6467cd8274": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d6ede7b0594a13a0359425ad67b8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9869b28705084a51be561ba311d3c4e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77383fdd36f4d85abe1420b5c3de4ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0260b0cccb044498d61d2bac8cad711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d026d49fb3a48d4b5cab585a8f0c9fd",
            "placeholder": "​",
            "style": "IPY_MODEL_f4a58c464f1140d98d84227eda96c739",
            "value": "STATIC_MODEL Models (success_label_5y):   0%"
          }
        },
        "bb7704d281f7456d91f75493b5ef4c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d14393c6899349c3b1b290ac9963624b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_679026781293439b9e7c2acd38a45f8a",
              "IPY_MODEL_75c6fca14181452399898f2737c63b2a",
              "IPY_MODEL_2343507107204f43a39c99cb68ca6dc0"
            ],
            "layout": "IPY_MODEL_636be7695e544e71a5d74ee382883ba8"
          }
        },
        "de8e1a38fc16457dbf839fe9878f3bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8bbf27b1b4c4eb2abf33d922d918aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0260b0cccb044498d61d2bac8cad711",
              "IPY_MODEL_3ae2692cc1e645da8e242248c3c06855",
              "IPY_MODEL_348aa6d854454666858261544993f341"
            ],
            "layout": "IPY_MODEL_96d6ede7b0594a13a0359425ad67b8fe"
          }
        },
        "ed90c401a15247c4bd9973bb489ec430": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4a58c464f1140d98d84227eda96c739": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa22d2f18f42438cb73385cf8e5ed8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
