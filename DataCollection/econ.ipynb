{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Data Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDERS\n",
    "OUTPUT_FOLDER = \"../Data/Output\"\n",
    "INPUT_FOLDER = \"../Data/Input\"\n",
    "\n",
    "INPUT_ECON_FOLDER = os.path.join(INPUT_FOLDER, \"Economic\")\n",
    "OUTPUT_ECON_FOLDER = os.path.join(OUTPUT_FOLDER, \"Economic\")\n",
    "\n",
    "fred_input_path = os.path.join(INPUT_ECON_FOLDER, \"fred_economic_data.csv\")\n",
    "bls_input_path = os.path.join(INPUT_ECON_FOLDER, \"bls_economic_data.csv\")\n",
    "yahoo_input_path = os.path.join(INPUT_ECON_FOLDER, \"yahoo_economic_data.csv\")\n",
    "econ_output_path = os.path.join(OUTPUT_ECON_FOLDER, \"economic_data.csv\")\n",
    "\n",
    "# Check contents of folders\n",
    "output_contents = os.listdir(OUTPUT_FOLDER)\n",
    "print(output_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "BLS_API_KEY = os.getenv(\"BLS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRED Data (API)\n",
    "https://fred.stlouisfed.org/docs/api/fred/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve FRED API key from environment variables\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "print(FRED_API_KEY)\n",
    "\n",
    "if not FRED_API_KEY:\n",
    "    raise ValueError(\"API key not found. Please set FRED_API_KEY in your .env file.\")\n",
    "\n",
    "# Base URL for FRED API\n",
    "FRED_BASE_URL = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "\n",
    "# Define economic indicators and series IDs\n",
    "FRED_SERIES = {\n",
    "    \"gdp_growth\": \"A191RL1Q225SBEA\",          # Real GDP growth rate (Quarterly)\n",
    "    \"interest_rate_fed_funds\": \"FEDFUNDS\",    # Federal Funds Rate (Monthly)\n",
    "    \"interest_rate_10y_treasury\": \"GS10\",     # 10-Year Treasury Yield (Daily)\n",
    "    \"money_supply_m2\": \"M2SL\",                # M2 Money Supply (Monthly)\n",
    "    \"yield_curve_10y_2y\": \"T10Y2Y\",           # 10Y-2Y Yield Curve Spread (Daily)\n",
    "    \"unemployment_rate\": \"UNRATE\",            # Unemployment Rate (Monthly)\n",
    "    \"cpi_inflation\": \"CPIAUCSL\",              # CPI Inflation Rate (Monthly)           # S&P 500 Index (Daily)\n",
    "    \"consumer_sentiment\": \"UMCSENT\",          # Consumer Sentiment Index (Monthly)\n",
    "    \"leading_economic_index\": \"USSLIND\",      # Leading Economic Index (Monthly)\n",
    "    \"corporate_bond_yield\": \"BAA\",            # Corporate Bond Yield (Monthly)\n",
    "    \"vix_index\": \"VIXCLS\"                     # VIX Index (Daily)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from FRED API\n",
    "def fetch_fred_series(series_id):\n",
    "    \"\"\"Fetch a single time-series from FRED API and return as DataFrame.\"\"\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": FRED_API_KEY,\n",
    "        \"file_type\": \"json\",  # Use JSON for easier parsing\n",
    "        \"sort_order\": \"asc\",   # Sort data in ascending order by date\n",
    "        \"start_date\": \"2004-01-01\",  # Specify a reasonable start date\n",
    "        \"end_data\": \"2025-01-01\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(FRED_BASE_URL, params=params, timeout=10)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "    # Extract and format data\n",
    "    data = response.json()[\"observations\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")  # Convert value to numeric\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    return df[\"value\"]  # Return only the series of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_fred_data():\n",
    "    # Fetch all series and combine into a single DataFrame\n",
    "    fred_data = {}\n",
    "    for key, series_id in FRED_SERIES.items():\n",
    "        print(f\"Fetching {key}, {series_id}...\")\n",
    "        fred_data[key] = fetch_fred_series(series_id)\n",
    "\n",
    "    df_fred = pd.DataFrame(fred_data)\n",
    "\n",
    "    return df_fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fred_data(df_fred):\n",
    "    df_fred.to_csv(fred_input_path, index=True)\n",
    "    print(f\"FRED data saved to {fred_input_path}\\n\")\n",
    "    print(df_fred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fred = get_all_fred_data()\n",
    "save_fred_data(df_fred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLS\n",
    "https://www.bls.gov/developers/home.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API URL\n",
    "BLS_API_URL = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\n",
    "\n",
    "# Define economic indicators from BLS\n",
    "BLS_SERIES = {\n",
    "    \"avg_hourly_earnings\": \"CES0500000003\",  # Average hourly earnings (Operational cost pressure)\n",
    "    \"ppi_final_demand\": \"WPSFD49207\",  # Producer Price Index (Final Demand)\n",
    "    \"job_openings\": \"JTS000000000000000JOL\",  # Job Openings (Labor market demand and hiring competition)\n",
    "    \"labor_force_participation_rate\": \"LNS11300000\", # Labor Force Participation Rate (Workforce availability and long-term growth potential),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bls_series(series_id, start_year=2004, end_year=2025):\n",
    "    \"\"\"\n",
    "    Fetch BLS series data using pagination for date ranges\n",
    "    \"\"\"\n",
    "    print(f\"Fetching series: {series_id}\")\n",
    "    \n",
    "    df_list = []\n",
    "    for year in range(start_year, end_year + 1, 20):\n",
    "        payload = {\n",
    "            \"seriesid\": [series_id],\n",
    "            \"startyear\": str(year),\n",
    "            \"endyear\": str(min(year + 19, end_year)),\n",
    "            \"registrationkey\": BLS_API_KEY\n",
    "        }\n",
    "\n",
    "        # Request data\n",
    "        response = requests.post(BLS_API_URL, json=payload)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "        json_data = response.json()\n",
    "        if \"Results\" not in json_data or \"series\" not in json_data[\"Results\"]:\n",
    "            continue\n",
    "\n",
    "        # Extract data points\n",
    "        data_points = json_data[\"Results\"][\"series\"][0][\"data\"]\n",
    "        if not data_points:\n",
    "            continue\n",
    "\n",
    "        formatted_data = {\n",
    "            pd.to_datetime(f\"{item['year']}-{item['period'][1:]}-01\"): float(item[\"value\"])\n",
    "            for item in data_points if item[\"period\"][0] == \"M\"  # Exclude annual averages\n",
    "        }\n",
    "\n",
    "        if formatted_data:\n",
    "            df_list.append(pd.Series(formatted_data))\n",
    "\n",
    "    if df_list:\n",
    "        return pd.concat(df_list).sort_index()\n",
    "    else:\n",
    "        return pd.Series(dtype=\"float64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bls_data():\n",
    "    bls_data = {}\n",
    "    for key, series_id in BLS_SERIES.items():\n",
    "        bls_data[key] = fetch_bls_series(series_id)\n",
    "\n",
    "    df_bls = pd.DataFrame(bls_data)\n",
    "    return df_bls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bls_data(df_bls):\n",
    "    df_bls.to_csv(bls_input_path)\n",
    "    print(f\"BLS data saved to {bls_input_path}\")\n",
    "    print()\n",
    "    print(df_bls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bls = fetch_bls_data()\n",
    "save_bls_data(df_bls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo\n",
    "https://github.com/ranaroussi/yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ETFs to track\n",
    "ETF_SYMBOLS = [\"SPY\", \"^GSPC\", \"XLK\", \"VHT\", \"PBW\", \"XLY\", \"IYT\", \"VOX\", \"IYZ\", \"VNQ\", \"FINX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_retries(symbol, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch ETF data with error handling and retries.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Fetching {symbol} (Attempt {attempt + 1})...\")\n",
    "            return yf.download(symbol, start=\"2004-01-01\", end=\"2025-12-31\", auto_adjust=False, progress=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {symbol}: {e}\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff delay\n",
    "\n",
    "    print(f\"Skipping {symbol} after {max_retries} failed attempts.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_etf_data(etf_symbols):\n",
    "    \"\"\"\n",
    "    Fetch and process ETF data into a DataFrame with relevant financial metrics.\n",
    "    \"\"\"\n",
    "    etf_data = []\n",
    "\n",
    "    for symbol in etf_symbols:\n",
    "        df = fetch_with_retries(symbol)\n",
    "        if df is not None and not df.empty:\n",
    "            df[\"Daily Return\"] = df[\"Adj Close\"].pct_change(fill_method=None)  # Daily return\n",
    "            df[\"Volatility\"] = df[\"Daily Return\"].rolling(30).std()  # 30-day rolling volatility\n",
    "            df[\"Momentum\"] = df[\"Adj Close\"].pct_change(14, fill_method=None).rolling(14).mean()  # Momentum (14-day)\n",
    "            df[\"50-Day MA\"] = df[\"Adj Close\"].rolling(50).mean()  # 50-day moving average\n",
    "            df[\"200-Day MA\"] = df[\"Adj Close\"].rolling(200).mean()  # 200-day moving average\n",
    "\n",
    "            df = df.rename(columns={\"Adj Close\": f\"Adj Close {symbol}\"})\n",
    "            # etf_data[symbol] = df[[f\"Adj Close {symbol}\", \"Daily Return\", \"Volatility\", \"Momentum\", \"50-Day MA\", \"200-Day MA\"]]\n",
    "\n",
    "            # Ensure Date is indexed correctly\n",
    "            df = df.reset_index()\n",
    "\n",
    "            # Insert ETF column at position 0 to properly label each row\n",
    "            df.insert(0, \"ETF\", symbol)\n",
    "\n",
    "            # Store only necessary columns\n",
    "            etf_data.append(df[[\"ETF\", \"Date\", \"Daily Return\", \n",
    "                                \"Volatility\", \"Momentum\", \"50-Day MA\", \"200-Day MA\", f\"Adj Close {symbol}\"]])\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "    df_combined = pd.concat(etf_data, ignore_index=True)\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yf_etf_data(df_etf):\n",
    "    df_etf.to_csv(yahoo_input_path, index=False)\n",
    "    print(f\"ETF data saved to {yahoo_input_path}\")\n",
    "    print()\n",
    "    print(df_etf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_etf = process_etf_data(ETF_SYMBOLS)\n",
    "save_yf_etf_data(df_etf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "fred = pd.read_csv(fred_input_path)\n",
    "bls = pd.read_csv(bls_input_path)\n",
    "yfinance = pd.read_csv(yahoo_input_path)\n",
    "\n",
    "# Standardizing column names: lowercase and replace spaces with underscores\n",
    "fred.columns = fred.columns.str.lower().str.replace(\" \", \"_\")\n",
    "bls.columns = bls.columns.str.lower().str.replace(\" \", \"_\")\n",
    "yfinance.columns = yfinance.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Convert date columns to datetime format if not already\n",
    "fred[\"date\"] = pd.to_datetime(fred[\"date\"])\n",
    "bls[\"date\"] = pd.to_datetime(bls[\"date\"])\n",
    "yfinance[\"date\"] = pd.to_datetime(yfinance[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FRED and BLS on \"date\"\n",
    "merged_df = pd.merge(fred, bls, on=\"date\", how=\"outer\")\n",
    "print(\"Merged FRED and BLS data\")\n",
    "\n",
    "# Merge YahooFinance based on \"date\" and \"ETF\"\n",
    "merged_df = pd.merge(merged_df, yfinance, on=\"date\", how=\"outer\")\n",
    "print(\"Merged YFinance data\")\n",
    "\n",
    "# Drop duplicate \"adj_close\" column if it appears multiple times\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "\n",
    "# Handle missing values (e.g., forward-fill for time-series continuity)\n",
    "merged_df.ffill(inplace=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "merged_df.to_csv(econ_output_path, index=False)\n",
    "print(f\"Cleaned economic data saved to {econ_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(econ_output_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile summary of all dataframes and output to txt file\n",
    "def compile_summary(filename, output_name):\n",
    "    df = pd.read_csv(filename)\n",
    "    PREVIEW_FOLDER = os.path.join(OUTPUT_FOLDER, \"Preview\")\n",
    "    output_file_path = os.path.join(PREVIEW_FOLDER, output_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(f\"DataFrame Summary\\n\")\n",
    "        f.write(f\"Number of rows: {df.shape[0]}\\n\")\n",
    "        f.write(f\"Number of columns: {df.shape[1]}\\n\")\n",
    "        f.write(\"Column names and info:\\n\")\n",
    "        df.info(buf=f)\n",
    "        f.write(\"\\nLast 50 rows:\\n\")\n",
    "        f.write(df.tail(n=50).to_string())\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Dataframe information saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_summary(econ_output_path, \"economic_data_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
