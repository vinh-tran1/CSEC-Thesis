{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDERS\n",
    "OUTPUT_FOLDER = \"../Data/Output\"\n",
    "INPUT_FOLDER = \"../Data/Input\"\n",
    "\n",
    "INPUT_CB_FOLDER = os.path.join(INPUT_FOLDER, \"CB_FINAL\")\n",
    "OUTPUT_CB_FOLDER = os.path.join(OUTPUT_FOLDER, \"Crunchbase\")\n",
    "OUTPUT_ECON_FOLDER = os.path.join(OUTPUT_FOLDER, \"Economic\")\n",
    "OUTPUT_NLP_FOLDER = os.path.join(OUTPUT_FOLDER, \"NLP\")\n",
    "OUTPUT_FINAL_FOLDER = os.path.join(OUTPUT_FOLDER, \"Final\")\n",
    "\n",
    "output_contents_cb = os.listdir(INPUT_CB_FOLDER)\n",
    "output_contents_econ = os.listdir(OUTPUT_ECON_FOLDER)\n",
    "output_contents_nlp = os.listdir(OUTPUT_NLP_FOLDER)\n",
    "\n",
    "print(output_contents_cb)\n",
    "print(output_contents_econ)\n",
    "print(output_contents_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Initial Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_df_path = os.path.join(INPUT_CB_FOLDER, \"cb_early_features.csv\")\n",
    "rounds_df_path = os.path.join(INPUT_CB_FOLDER, \"cb_post_funding_rounds.csv\")\n",
    "outcomes_df_path = os.path.join(INPUT_CB_FOLDER, \"cb_outcomes.csv\")\n",
    "\n",
    "early_df = pd.read_csv(early_df_path)\n",
    "rounds_df = pd.read_csv(rounds_df_path)\n",
    "outcomes_df = pd.read_csv(outcomes_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_df.info()\n",
    "rounds_df.info()\n",
    "outcomes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Econ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df_path = os.path.join(OUTPUT_ECON_FOLDER, \"economic_data.csv\")\n",
    "econ_df = pd.read_csv(econ_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_df_path = os.path.join(OUTPUT_NLP_FOLDER, \"perplexity.csv\")\n",
    "nlp_df = pd.read_csv(nlp_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crunchbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "SUCCESS_YEARS_THRESHOLD = 5\n",
    "\n",
    "# top unviversities based on most startup founders\n",
    "TOP_SCHOOLS = [ \n",
    "    \"Stanford University\", \"Stanford\",\n",
    "    \"University of California, Berkeley\", \"UC Berkeley\", \"Berkeley\",\n",
    "    \"Harvard University\", \"Harvard\",\n",
    "    \"Massachusetts Institute of Technology\", \"MIT\",\n",
    "    \"University of Pennsylvania\", \"UPenn\", \"Penn\",\n",
    "    \"Cornell University\", \"Cornell\",\n",
    "    \"University of Michigan, Ann Arbor\", \"University of Michigan\", \"Michigan\", \"UMich\",\n",
    "    \"University of Texas at Austin\", \"UT Austin\",\n",
    "    \"Yale University\", \"Yale\",\n",
    "    \"University of California, Los Angeles\", \"UCLA\", \"Los Angeles\",\n",
    "    \"Princeton University\", \"Princeton\",\n",
    "    \"Columbia University\", \"Columbia\",\n",
    "    \"University of Illinois, Urbana-Champaign\", \"UIUC\", \"Illinois\",\n",
    "    \"University of Southern California\", \"USC\", \"Southern California\",\n",
    "    \"University of Wisconsin-Madison\", \"UW Madison\", \"Wisconsin\",\n",
    "    \"New York University\", \"NYU\",\n",
    "    \"Brown University\", \"Brown\",\n",
    "    \"Duke University\", \"Duke\",\n",
    "    \"Carnegie Mellon University\", \"Carnegie Mellon\", \"CMU\",\n",
    "    \"Northwestern University\", \"Northwestern\"\n",
    "]\n",
    "\n",
    "CS_ENGINEERING_DEGREES = [\n",
    "    \"Computer Science\", \"CS\", \"Data Science\", \"Software\", \"Engineering\", \"Finance\", \"Business Administration\"\n",
    "]\n",
    "\n",
    "# RELEVANT_POSTIONS = ['founder', 'ceo', 'chief executive officer', 'cto',\n",
    "#                     'chief technology officer', 'coo', 'chief operating officer',\n",
    "#                     'cfo', 'chief financial officer', 'cso', 'chief strategy officer']\n",
    "\n",
    "RELEVANT_POSTIONS = ['founder']\n",
    "\n",
    "STARTUP_HUB_CITIES = [\n",
    "    \"San Francisco\", \"New York\", \"Boston\", \"Los Angeles\", \"Austin\"\n",
    "]\n",
    "\n",
    "TOP_VC = [\n",
    "    \"Sequoia Capital\", \"Sequoia\",\n",
    "    \"Andreessen Horowitz\", \"a16z\",\n",
    "    \"Accel\", \"Accel Partners\",\n",
    "    \"Tiger Global Management\", \"Tiger Global\",\n",
    "    \"Lightspeed Venture Partners\", \"Lightspeed\",\n",
    "    \"Benchmark\", \"Benchmark Capital\",\n",
    "    \"New Enterprise Associates\", \"NEA\",\n",
    "    \"Bessemer Venture Partners\", \"Bessemer\",\n",
    "    \"Kleiner Perkins\", \"KPCB\",\n",
    "    \"Greylock Partners\", \"Greylock\",\n",
    "    \"Index Ventures\",\n",
    "    \"General Catalyst\", \"General Catalyst Partners\",\n",
    "    \"Founders Fund\",\n",
    "    \"First Round Capital\", \"First Round\",\n",
    "    \"Y Combinator\", \"YC\",\n",
    "    \"Peter Thiel\",\n",
    "    \"Reid Hoffman\",\n",
    "    \"Elon Musk\",\n",
    "    \"Marc Andreessen\",\n",
    "    \"Chris Sacca\",\n",
    "    \"Paul Graham\",\n",
    "    \"Ron Conway\",\n",
    "    \"Naval Ravikant\",\n",
    "    \"Max Levchin\",\n",
    "    \"David Sacks\",\n",
    "    \"Vinod Khosla\",\n",
    "    \"Paul Buchheit\",\n",
    "    \"Jessica Livingston\",\n",
    "    \"Steve Anderson\",\n",
    "    \"Aydin Senkut\"\n",
    "]\n",
    "\n",
    "VALID_ACTIVE_STATUSES = ['operating', 'ipo', 'acquired']\n",
    "\n",
    "EARLY_FUNDING_TYPES = {\n",
    "    'series_a', 'series_b', 'series_c', 'series_d',\n",
    "    'series_e', 'series_f', 'series_g', 'series_h',\n",
    "    'series_i', 'series_j', 'series_unknown', 'corporate_round'\n",
    "}\n",
    "\n",
    "VALID_GENDER_LABELS = {\n",
    "    'male', 'female', 'ftm', 'androgyne', 'transgender_woman',\n",
    "    'non_binary', 'agender', 'transgender_person', 'other',\n",
    "    'androgynous', 'genderqueer', 'gender_nonconforming'\n",
    "}\n",
    "\n",
    "bucket_to_categories = {\n",
    "    \"Life Sciences\": [\n",
    "        \"Bioinformatics\", \"Biopharma\", \"Biotechnology\", \"Health Care\",\n",
    "        \"Health Diagnostics\", \"Medical\", \"Medical Device\", \"Pharmaceutical\",\n",
    "        \"Life Science\", \"Wearables\"\n",
    "    ],\n",
    "    \"Fintech\": [\n",
    "        \"Finance\", \"Financial Services\", \"FinTech\", \"Health Insurance\", \"InsurTech\"\n",
    "    ],\n",
    "    \"Consumer Goods\": [\n",
    "        \"Apparel\", \"Consumer Electronics\", \"E-Commerce\", \"Fashion\", \"Retail\",\n",
    "        \"Retail Technology\", \"Food and Beverage\"\n",
    "    ],\n",
    "    \"Technology\": [\n",
    "        \"Cloud Computing\", \"DevOps\", \"Enterprise Software\", \"Information Technology\",\n",
    "        \"Legal\", \"Marketing\", \"Mobile\", \"Mobile Apps\", \"SaaS\", \"PaaS\", \"Sales\", \"Software\", \"Blockchain\", \"Cryptocurrency\", \"Web3\",\n",
    "        \"Artificial Intelligence (AI)\", \"Data Management\", \"Machine Learning\", \"Natural Language Processing\", \n",
    "        \"Electronics\", \"Industrial Automation\", \"Internet of Things\",  \"Robotics\", \"Semiconductor\", \"Sensor\", \"Supply Chain Management\",\n",
    "        \"Cyber Security\", \"Security\", \"Drones\", \"Hardware\",\n",
    "        \"EdTech\", \"Nanotechnology\"\n",
    "    ],\n",
    "    \"Cleantech\": [\n",
    "        \"Battery\", \"Clean Energy\", \"CleanTech\", \"Energy\",\n",
    "        \"Environmental Consulting\", \"GreenTech\", \"Oil and Gas\", \"Solar\"\n",
    "    ],\n",
    "    \"Transportation\": [\n",
    "        \"Aerospace\", \"Automotive\", \"Electric Vehicle\",\n",
    "        \"Logistics\", \"Space Travel\", \"Transportation\", \"Travel\"\n",
    "    ],\n",
    "    \"Media Entertainment and Gaming\": [\n",
    "        \"Augmented Reality\", \"Gaming\", \"Media and Entertainment\", \"Online Games\",\n",
    "        \"Social Media\", \"Sports\", \"Video Games\", \"Video Streaming\", \"Virtual Reality\"\n",
    "    ],\n",
    "    \"Telecom\": [\n",
    "        \"Telecommunications\"\n",
    "    ],\n",
    "    \"Real Estate\": [\n",
    "        \"PropTech\", \"Real Estate\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_col = early_df['org_uuid'].nunique()\n",
    "print(num_unique_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Double check that only funding rounds within 5 years of funding\n",
    "\"\"\"\n",
    "# Ensure date columns are parsed first\n",
    "early_df['announced_on'] = pd.to_datetime(early_df['announced_on'], errors='coerce')\n",
    "early_df['founded_on'] = pd.to_datetime(early_df['founded_on'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN dates\n",
    "early_df = early_df.dropna(subset=['founded_on'])\n",
    "\n",
    "# Reapply 5-year cutoff as a redundancy to ensure early_df is clean\n",
    "early_df = early_df[\n",
    "    early_df['announced_on'].isna() |\n",
    "    ((early_df['announced_on'] - early_df['founded_on']).dt.days <= 1825)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_industries(category_str, mapping_dict):\n",
    "    \"\"\"\n",
    "    Drops startups with no relevant industry match, create industry column\n",
    "    \"\"\"\n",
    "    if pd.isna(category_str):\n",
    "        return []\n",
    "    categories = [x.strip() for x in category_str.split(',')]\n",
    "    matched = []\n",
    "    for key, terms in mapping_dict.items():\n",
    "        if any(cat in terms for cat in categories):\n",
    "            matched.append(key)\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_founder_role(title):\n",
    "    \"\"\"\n",
    "    Filter for executive job roles and drop everything else\n",
    "    \"\"\"\n",
    "    if isinstance(title, str):\n",
    "        title = title.lower()\n",
    "        return any(k in title for k in RELEVANT_POSTIONS)\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_founder_of_org(description, org_name):\n",
    "    if not isinstance(description, str) or not isinstance(org_name, str):\n",
    "        return False\n",
    "\n",
    "    desc = description.lower()\n",
    "    org = re.escape(org_name.lower())  # escape for special chars like '+' or '&'\n",
    "\n",
    "    # Tokenize description (split on non-word chars to handle punctuation)\n",
    "    tokens = re.split(r'\\W+', desc)\n",
    "    \n",
    "    # Iterate through positions where 'founder' appears\n",
    "    founder_indices = [i for i, word in enumerate(tokens) if 'founder' in word]\n",
    "\n",
    "    for i in founder_indices:\n",
    "        # Define the window around \"founder\"\n",
    "        window_start = max(i - 3, 0)\n",
    "        window_end = i + 4  # inclusive of 5 after\n",
    "\n",
    "        context_window = tokens[window_start:window_end]\n",
    "        if any(org in token for token in context_window):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "string = 'Kevin is the CEO and cofounder of Instagram a community of more than 300 million who capture and share the worlds moments on the service He is responsible for the companys overall vision and strategy as well as daytoday operations   Prior to founding Instagram Kevin was part of the startup Odeo which later became Twitter and spent two years at Google working on products such as Gmail and Google Reader He graduated from Stanford University with a bachelor of science in management science and engineering'\n",
    "print(is_founder_of_org(string, \"Instagram\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# founders_df = early_df[early_df['featured_job_title'].apply(is_founder_role)].copy()\n",
    "# founders_from_desc = early_df[\n",
    "#     early_df['person_description'].apply(has_founder_in_description)\n",
    "# ].copy()\n",
    "# founders_df = pd.concat([founders_df, founders_from_desc], ignore_index=True).drop_duplicates(subset=['org_uuid', 'person_uuid'])\n",
    "# valid_orgs = founders_df['org_uuid'].unique()\n",
    "\n",
    "\n",
    "# Title-based founder rows\n",
    "founders_from_title = early_df[\n",
    "    early_df['featured_job_title'].apply(is_founder_role)\n",
    "]\n",
    "\n",
    "# Description-based founder rows\n",
    "founders_from_desc = early_df[\n",
    "    early_df.apply(lambda row: is_founder_of_org(row['person_description'], row['org_name']), axis=1)\n",
    "]\n",
    "\n",
    "# Combine and deduplicate by person and org\n",
    "founders_df = pd.concat([founders_from_title, founders_from_desc], ignore_index=True)\n",
    "founders_df = founders_df.drop_duplicates(subset=['org_uuid', 'person_uuid'])\n",
    "\n",
    "# Keep only founders with description, gender, and either school or degree\n",
    "filtered_founders = founders_df[\n",
    "    founders_df['person_description'].notna() &\n",
    "    founders_df['person_gender'].notna() &\n",
    "    (\n",
    "        founders_df['institution_name'].notna() |\n",
    "        founders_df['subject'].notna() |\n",
    "        founders_df['degree_type'].notna()\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "# Now collect orgs with at least one true founder\n",
    "valid_orgs = filtered_founders['org_uuid'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for companies with non-null organization descriptions\n",
    "early_df = early_df[early_df['org_description'].notna() & (early_df['org_description'].str.strip() != '')].copy()\n",
    "\n",
    "# map industries to 9 main categories\n",
    "early_df['matched_industries'] = early_df['category_list'].apply(lambda x: map_industries(x, bucket_to_categories))\n",
    "early_df = early_df[early_df['matched_industries'].map(len) > 0].copy()\n",
    "early_df['industry'] = early_df['matched_industries'].apply(lambda x: ','.join(set(x)))\n",
    "\n",
    "# anchored list\n",
    "base_orgs = early_df[['org_uuid', 'org_name', 'city', 'founded_on', 'industry', 'org_description']].drop_duplicates('org_uuid')\n",
    "\n",
    "early_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_col = early_df['org_uuid'].nunique()\n",
    "print(num_unique_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feat Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Function\n",
    "\"\"\"\n",
    "EXCLUDE_GENDER_LABELS = {'unknown', 'not_provided', 'none', '', None, np.nan}\n",
    "\n",
    "# Shannon entropy\n",
    "# p = proportion of founders in gender category i\n",
    "# H is higher when gender distribution is more uniform (diverse)\n",
    "def compute_gender_diversity(genders):\n",
    "    clean_genders = [\n",
    "        str(g).strip().lower()\n",
    "        for g in genders\n",
    "        if str(g).strip().lower() not in EXCLUDE_GENDER_LABELS\n",
    "    ]\n",
    "    if not clean_genders:\n",
    "        return None\n",
    "    \n",
    "    counts = pd.Series(clean_genders).value_counts(normalize=True)\n",
    "    probs = counts.values\n",
    "    entropy = -np.sum(probs * np.log(probs))\n",
    "    return entropy / np.log(len(probs)) if len(probs) > 1 else 0.0 # normalized entropy (0–1), only if multiple distinct categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Founder and Education Features\n",
    "\"\"\"\n",
    "# Aggregate founder-level features at org level\n",
    "founder_feats = filtered_founders.groupby('org_uuid').agg(\n",
    "    founder_count=('person_uuid', 'nunique'),\n",
    "    founder_gender_diversity=('person_gender', compute_gender_diversity),\n",
    "    known_gender_count=('person_gender', lambda x: sum(pd.notna(x))),\n",
    "    has_top_school_founder=('institution_name', lambda x: any(\n",
    "        str(inst).strip() in TOP_SCHOOLS for inst in x if pd.notna(inst)\n",
    "    )),\n",
    "    num_optimal_degrees=('subject', lambda x: sum([\n",
    "        1 if any(cs.lower() in str(subj).strip().lower() for cs in CS_ENGINEERING_DEGREES) else 0\n",
    "        for subj in x if pd.notna(subj)\n",
    "    ]))\n",
    ").reset_index()\n",
    "\n",
    "# Normalize optimal degree count per founder\n",
    "founder_feats['optimal_degree_ratio'] = founder_feats['num_optimal_degrees'] / founder_feats['founder_count']\n",
    "\n",
    "# is_repeat_founder\n",
    "repeat_counts = early_df.groupby('person_uuid')['org_uuid'].nunique().reset_index()\n",
    "repeat_founders = repeat_counts[repeat_counts['org_uuid'] > 1]['person_uuid']\n",
    "early_df['is_repeat_founder'] = early_df['person_uuid'].isin(repeat_founders)\n",
    "repeat_founder_df = early_df.groupby('org_uuid')['is_repeat_founder'].max().reset_index()\n",
    "\n",
    "# Concatenate all founder descriptions into one string per org\n",
    "founder_desc_df = filtered_founders.groupby('org_uuid')['person_description'].apply(\n",
    "    lambda x: ' '.join([str(s) for s in x if pd.notna(s)])\n",
    ").reset_index().rename(columns={'person_description': 'founder_description_blob'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Founding Timing Features\n",
    "\"\"\"\n",
    "early_df['founded_on'] = pd.to_datetime(early_df['founded_on'], errors='coerce')\n",
    "early_df['announced_on'] = pd.to_datetime(early_df['announced_on'], errors='coerce')\n",
    "\n",
    "# First funding round timing\n",
    "funding_group = early_df.groupby('org_uuid')['announced_on'].min().reset_index()\n",
    "funding_group = funding_group.rename(columns={'announced_on': 'first_funding_date'})\n",
    "\n",
    "timing_df = early_df[['org_uuid', 'founded_on']].drop_duplicates().merge(funding_group, on='org_uuid', how='left')\n",
    "timing_df['age_at_first_funding'] = (timing_df['first_funding_date'] - timing_df['founded_on']).dt.days / 30\n",
    "timing_df['first_funding_delay'] = timing_df['age_at_first_funding']\n",
    "timing_df['founded_year'] = timing_df['founded_on'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Early Funding Dynamics\n",
    "\"\"\"\n",
    "# Ensure datetime formatting\n",
    "early_df['announced_on'] = pd.to_datetime(early_df['announced_on'], errors='coerce')\n",
    "early_df['founded_on'] = pd.to_datetime(early_df['founded_on'], errors='coerce')\n",
    "\n",
    "# Base: one row per unique funding round\n",
    "unique_early_rounds = early_df.drop_duplicates(subset='funding_round_uuid').copy()\n",
    "\n",
    "# --- Months since founding for each early round ---\n",
    "unique_early_rounds['months_since_founding'] = (\n",
    "    (unique_early_rounds['announced_on'] - unique_early_rounds['founded_on']).dt.days / 30\n",
    ")\n",
    "\n",
    "# --- Calculate Early Series Count ---\n",
    "print(\"Calculating early series count...\")\n",
    "early_series_count_df = unique_early_rounds.groupby('org_uuid').size().reset_index(name='early_series_count')\n",
    "\n",
    "# --- Calculate Average Time To an Early Round (Corrected Name) ---\n",
    "print(\"Calculating average time to early round...\")\n",
    "avg_time_to_round_df = unique_early_rounds.groupby('org_uuid')['months_since_founding'].mean().reset_index(name='avg_time_to_early_round_months')\n",
    "\n",
    "# --- Calculate Average Time BETWEEN Early Rounds (Correct Logic) ---\n",
    "print(\"Calculating average time between early rounds...\")\n",
    "unique_early_rounds = unique_early_rounds.sort_values(['org_uuid', 'announced_on'])\n",
    "unique_early_rounds['time_diff_months'] = unique_early_rounds.groupby('org_uuid')['months_since_founding'].diff()\n",
    "avg_time_between_df = unique_early_rounds.dropna(subset=['time_diff_months']) \\\n",
    "    .groupby('org_uuid')['time_diff_months'].mean().reset_index(name='avg_time_between_rounds')\n",
    "\n",
    "# --- Has Any Early Rounds ---\n",
    "valid_rounds = unique_early_rounds.dropna(subset=['announced_on'])[['org_uuid']]\n",
    "has_funding_df = valid_rounds.groupby('org_uuid').size().reset_index(name='num_early_rounds')\n",
    "has_funding_df['has_funding_data'] = True\n",
    "\n",
    "# Ensure all orgs represented\n",
    "all_orgs = early_df[['org_uuid']].drop_duplicates()\n",
    "has_funding_df = all_orgs.merge(has_funding_df, on='org_uuid', how='left')\n",
    "has_funding_df['num_early_rounds'] = has_funding_df['num_early_rounds'].fillna(0).astype(int)\n",
    "has_funding_df['has_funding_data'] = has_funding_df['has_funding_data'].fillna(False)\n",
    "\n",
    "# --- Disclosed Funding ---\n",
    "valid_amounts = unique_early_rounds.dropna(subset=['raised_amount_usd'])[['org_uuid', 'raised_amount_usd']]\n",
    "known_amounts_df = valid_amounts.groupby('org_uuid').size().reset_index(name='num_disclosed_rounds')\n",
    "known_amounts_df['has_disclosed_funding'] = True\n",
    "\n",
    "known_amounts_df = all_orgs.merge(known_amounts_df, on='org_uuid', how='left')\n",
    "known_amounts_df['num_disclosed_rounds'] = known_amounts_df['num_disclosed_rounds'].fillna(0).astype(int)\n",
    "known_amounts_df['has_disclosed_funding'] = known_amounts_df['has_disclosed_funding'].fillna(False)\n",
    "\n",
    "# --- Known Investor Flag ---\n",
    "print(\"Checking for known investors...\")\n",
    "def has_known_investor(investors_str, top_vcs):\n",
    "    if pd.isna(investors_str):\n",
    "        return False\n",
    "    return any(vc.lower() in investors_str.lower() for vc in top_vcs)\n",
    "\n",
    "investors_per_org = early_df.groupby('org_uuid')['investor_name'].apply(\n",
    "    lambda x: ' '.join(str(s) for s in x if pd.notna(s))\n",
    ").reset_index()\n",
    "\n",
    "investors_per_org['has_known_investor'] = investors_per_org['investor_name'].apply(\n",
    "    lambda x: has_known_investor(x, TOP_VC)\n",
    ")\n",
    "\n",
    "known_investor_df = investors_per_org[['org_uuid', 'has_known_investor']]\n",
    "\n",
    "# --- Total Unique Investors ---\n",
    "early_unique_inv = early_df[['org_uuid', 'funding_round_uuid', 'investor_count']].drop_duplicates()\n",
    "investor_count_df = early_unique_inv.groupby('org_uuid')['investor_count'].sum(min_count=1).reset_index()\n",
    "\n",
    "# --- Funding Velocity (within first 5 years) ---\n",
    "print(\"Calculating funding velocity...\")\n",
    "def compute_funding_velocity(df, time_window_months=60):\n",
    "    df = df.drop_duplicates(subset='funding_round_uuid').copy()\n",
    "    df['months_since_founding'] = (df['announced_on'] - df['founded_on']).dt.days / 30\n",
    "    within_window = df[df['months_since_founding'] <= time_window_months][['org_uuid', 'announced_on']].drop_duplicates()\n",
    "    velocity_df = within_window.groupby('org_uuid').size().reset_index(name='early_rounds_5yr')\n",
    "    velocity_df['funding_velocity'] = velocity_df['early_rounds_5yr'] / (time_window_months / 12)  # rounds/year\n",
    "    return velocity_df[['org_uuid', 'funding_velocity']]\n",
    "\n",
    "funding_velocity_df = compute_funding_velocity(early_df)\n",
    "\n",
    "# --- First Funding Amount Buckets ---\n",
    "print(\"Bucketizing first funding amounts...\")\n",
    "def bucketize_funding_amounts(df):\n",
    "    df = df.drop_duplicates(subset='funding_round_uuid')\n",
    "    df = df.sort_values('announced_on')\n",
    "    first_funding = df.groupby('org_uuid').first().reset_index()\n",
    "    buckets = pd.cut(\n",
    "        first_funding['raised_amount_usd'],\n",
    "        bins=[-1, 1e5, 5e5, 2e6, 1e7, np.inf],\n",
    "        labels=['<$100K', '$100K-500K', '$500K-2M', '$2M-10M', '$10M+']\n",
    "    )\n",
    "    return pd.DataFrame({\n",
    "        'org_uuid': first_funding['org_uuid'],\n",
    "        'first_funding_amount_bucket': buckets\n",
    "    })\n",
    "\n",
    "funding_bucket_df = bucketize_funding_amounts(\n",
    "    early_df[['org_uuid', 'announced_on', 'raised_amount_usd', 'funding_round_uuid']].dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Burn Rate: # total EARLY funding / months active\n",
    "\"\"\"\n",
    "# Ensure dates are datetime\n",
    "early_df['announced_on'] = pd.to_datetime(early_df['announced_on'], errors='coerce')\n",
    "early_df['founded_on'] = pd.to_datetime(early_df['founded_on'], errors='coerce')\n",
    "\n",
    "# Step 1: drop duplicate funding_round_uuids (keep earliest instance)\n",
    "early_unique = early_df.drop_duplicates(subset='funding_round_uuid').copy()\n",
    "\n",
    "# Step 2: sum early funding (now deduplicated)\n",
    "early_unique['raised_amount_usd'] = early_unique['raised_amount_usd'].fillna(0)\n",
    "early_funding_sum = early_unique.groupby('org_uuid')['raised_amount_usd'].sum().reset_index(name='total_early_funding_usd')\n",
    "\n",
    "# Step 3: compute last early round date (deduplicated)\n",
    "last_early_round = early_unique.groupby('org_uuid')['announced_on'].max().reset_index(name='last_early_round_date')\n",
    "\n",
    "# Step 4: compute months active\n",
    "burn_timing = early_unique[['org_uuid', 'founded_on']].drop_duplicates().merge(\n",
    "    last_early_round, on='org_uuid', how='left'\n",
    ")\n",
    "burn_timing['active_months'] = (\n",
    "    (burn_timing['last_early_round_date'] - burn_timing['founded_on']).dt.days / 30\n",
    ")\n",
    "\n",
    "# Step 5: calculate burn rate\n",
    "burn_rate_df = burn_timing.merge(early_funding_sum, on='org_uuid', how='left')\n",
    "burn_rate_df['burn_rate'] = burn_rate_df['total_early_funding_usd'] / burn_rate_df['active_months']\n",
    "burn_rate_df = burn_rate_df[['org_uuid', 'burn_rate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Geography\n",
    "\"\"\"\n",
    "# Normalize hub cities\n",
    "STARTUP_HUB_CITIES = [c.strip().lower() for c in STARTUP_HUB_CITIES]\n",
    "\n",
    "# Substring match function\n",
    "def is_in_startup_hub(city):\n",
    "    if pd.isna(city):\n",
    "        return False\n",
    "    city = str(city).strip().lower()\n",
    "    return any(hub in city for hub in STARTUP_HUB_CITIES)\n",
    "\n",
    "# Apply to DataFrame\n",
    "early_df['is_startup_hub'] = early_df['city'].apply(is_in_startup_hub)\n",
    "\n",
    "# Extract final startup-level flag\n",
    "startup_hub_df = early_df[['org_uuid', 'is_startup_hub']].drop_duplicates()\n",
    "\n",
    "# Is startup in city that is startup rich\n",
    "# density = # startups founded in same city + year\n",
    "cohort = early_df[['org_uuid', 'city', 'founded_on']].drop_duplicates()\n",
    "cohort['founded_year'] = pd.to_datetime(cohort['founded_on'], errors='coerce').dt.year\n",
    "cohort_density = cohort.groupby(['city', 'founded_year']).size().reset_index(name='cohort_funding_density')\n",
    "cohort = cohort.merge(cohort_density, on=['city', 'founded_year'], how='left')\n",
    "cohort_density_df = cohort[['org_uuid', 'cohort_funding_density']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_success_labels(early_df, rounds_df, outcomes_df, SUCCESS_YEARS_1=5, SUCCESS_YEARS_2=10):\n",
    "    # Clean datetime\n",
    "    outcomes_df['went_public_on'] = pd.to_datetime(outcomes_df['went_public_on'], errors='coerce')\n",
    "    outcomes_df['acquired_on'] = pd.to_datetime(outcomes_df['acquired_on'], errors='coerce')\n",
    "    early_df['founded_on'] = pd.to_datetime(early_df['founded_on'], errors='coerce')\n",
    "    rounds_df['funding_round_date'] = pd.to_datetime(rounds_df['funding_round_date'], errors='coerce')\n",
    "\n",
    "    # Base outcome type from status (operating, closed, ipo, acquisition)\n",
    "    org_df = early_df[['org_uuid', 'founded_on']].copy()\n",
    "    org_df = org_df.merge(outcomes_df[['org_uuid', 'status']], on='org_uuid', how='left')\n",
    "    df = org_df.copy()\n",
    "    df['outcome_type'] = df['status'].str.lower()\n",
    "\n",
    "    # Define IPO/acquisition outcome_type\n",
    "    outcomes_df['outcome_type'] = outcomes_df.apply(\n",
    "        lambda row: 'ipo' if pd.notnull(row['went_public_on'])\n",
    "        else ('acquisition' if pd.notnull(row['acquired_on']) else None),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # IPO/acquisition: always success (anytime)\n",
    "    exit_success_all = outcomes_df[outcomes_df['outcome_type'].isin(['ipo', 'acquisition'])][['org_uuid', 'outcome_type']].drop_duplicates()\n",
    "    df = df.drop(columns=['outcome_type'], errors='ignore')  # remove preassigned outcome_type to override with correct priority\n",
    "    df = df.merge(exit_success_all, on='org_uuid', how='left')\n",
    "\n",
    "    # Series A+ within X years and company still operating\n",
    "    def series_success_within(years):\n",
    "        df_funding = rounds_df[rounds_df['investment_type'].str.lower().isin(EARLY_FUNDING_TYPES)].copy()\n",
    "        df_funding = df_funding.merge(org_df[['org_uuid', 'founded_on', 'status']], on='org_uuid', how='left')\n",
    "        df_funding['years_since_founding'] = (df_funding['funding_round_date'] - df_funding['founded_on']).dt.days / 365\n",
    "        df_funding = df_funding[\n",
    "            (df_funding['years_since_founding'] <= years) &\n",
    "            (df_funding['status'].str.lower().isin(VALID_ACTIVE_STATUSES))\n",
    "        ]\n",
    "        return df_funding[['org_uuid']].drop_duplicates().assign(outcome_type='funding')\n",
    "\n",
    "    # Compute Series A+ outcomes\n",
    "    funding_5y = series_success_within(SUCCESS_YEARS_1)\n",
    "    funding_10y = series_success_within(SUCCESS_YEARS_2)\n",
    "\n",
    "    # Assign funding-based outcome type if not already success\n",
    "    df.loc[df['org_uuid'].isin(funding_5y['org_uuid']) & df['outcome_type'].isna(), 'outcome_type'] = 'funded_no_exit'\n",
    "\n",
    "    # Label too-young companies founded after Jan 1, 2020\n",
    "    df['too_young'] = df['founded_on'] > pd.to_datetime(\"2020-01-01\")\n",
    "    df.loc[df['too_young'] & df['outcome_type'].isna(), 'outcome_type'] = 'too_young'\n",
    "\n",
    "    # Assign 'still_active' if company is operating, older than 5y, and has no success\n",
    "    five_year_cutoff = pd.to_datetime(\"2025-01-01\") - timedelta(days=SUCCESS_YEARS_1 * 365)\n",
    "    is_old_enough = df['founded_on'] < five_year_cutoff\n",
    "    is_operating = df['status'].str.lower().isin(VALID_ACTIVE_STATUSES)\n",
    "    no_outcome = df['outcome_type'].isna()\n",
    "\n",
    "    df.loc[no_outcome & is_operating & is_old_enough, 'outcome_type'] = 'still_active'\n",
    "\n",
    "    # Anything else left must be closed (no success, not too young, not still operating)\n",
    "    df['outcome_type'] = df['outcome_type'].fillna('closed')\n",
    "\n",
    "    # Final success labels\n",
    "    success_5y_ids = pd.concat([exit_success_all, funding_5y])['org_uuid'].unique()\n",
    "    success_10y_ids = pd.concat([exit_success_all, funding_10y])['org_uuid'].unique()\n",
    "    df['success_label_5y'] = df['org_uuid'].isin(success_5y_ids)\n",
    "    df['success_label_10y'] = df['org_uuid'].isin(success_10y_ids)\n",
    "\n",
    "    return df[['org_uuid', 'success_label_5y', 'success_label_10y', 'outcome_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_df = compute_success_labels(early_df, rounds_df, outcomes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = base_orgs.copy()\n",
    "final = final[final['org_uuid'].isin(valid_orgs)].copy()\n",
    "\n",
    "# Founders\n",
    "final = final.merge(founder_feats, on='org_uuid', how='left')\n",
    "final = final.merge(repeat_founder_df, on='org_uuid', how='left')\n",
    "final = final.merge(founder_desc_df, on='org_uuid', how='left')\n",
    "\n",
    "# Define default fills (EXCLUDE founder_gender_diversity and known_gender_count)\n",
    "founder_fill_defaults = {\n",
    "    'founder_count': 0,\n",
    "    'has_top_school_founder': False,\n",
    "    'num_optimal_degrees': 0,\n",
    "    'optimal_degree_ratio': 0.0,\n",
    "    'is_repeat_founder': False\n",
    "}\n",
    "final = final.fillna(value=founder_fill_defaults)\n",
    "\n",
    "# Convert to int after fill\n",
    "final['founder_count'] = final['founder_count'].astype(int)\n",
    "\n",
    "# Track missingness\n",
    "final['founder_gender_missing'] = final['founder_gender_diversity'].isna()\n",
    "final['founder_degree_missing'] = final['num_optimal_degrees'] == 0\n",
    "final['founder_school_missing'] = final['has_top_school_founder'] == False\n",
    "final['founder_desc_missing'] = final['founder_description_blob'].str.strip() == ''\n",
    "\n",
    "# if we want, drop rows with null values in all founder-related features\n",
    "# keep only rows with a valid founder description AND at least one structured founder attribute\n",
    "# final = final[\n",
    "#     (~final['founder_desc_missing']) &\n",
    "#     ~(\n",
    "#         final['founder_gender_missing'] &\n",
    "#         final['founder_degree_missing'] &\n",
    "#         final['founder_school_missing']\n",
    "#     )\n",
    "# ].copy()\n",
    "\n",
    "# final = final[\n",
    "#     (~final['founder_desc_missing']) &                         # must have description\n",
    "#     (~final['founder_gender_missing']) &                      # must have gender\n",
    "#     (~final['founder_degree_missing'] | ~final['founder_school_missing'])  # must have degree OR school\n",
    "# ].copy()\n",
    "\n",
    "# final = final[\n",
    "#     ~final['founder_desc_missing']\n",
    "# ].copy()\n",
    "\n",
    "# timing\n",
    "final = final.merge(timing_df[['org_uuid', 'founded_year', 'age_at_first_funding', 'first_funding_delay']], on='org_uuid', how='left')\n",
    "# final = final.merge(early_counts, on='org_uuid', how='left')\n",
    "# final = final.merge(burn_rate_df, on='org_uuid', how='left')\n",
    "# final = final.merge(has_funding_df, on='org_uuid', how='left')\n",
    "# final = final.merge(known_amounts_df, on='org_uuid', how='left')\n",
    "# final = final.merge(funding_velocity_df, on='org_uuid', how='left')\n",
    "# final = final.merge(funding_bucket_df, on='org_uuid', how='left')\n",
    "\n",
    "# Merge all early funding features at once\n",
    "early_funding_features_df = (\n",
    "    early_series_count_df\n",
    "    .merge(avg_time_to_round_df, on='org_uuid', how='outer')\n",
    "    .merge(avg_time_between_df, on='org_uuid', how='outer')\n",
    "    .merge(has_funding_df, on='org_uuid', how='outer')\n",
    "    .merge(known_amounts_df, on='org_uuid', how='outer')\n",
    "    .merge(funding_velocity_df, on='org_uuid', how='outer')\n",
    "    .merge(funding_bucket_df, on='org_uuid', how='outer')\n",
    ")\n",
    "\n",
    "# Then merge into final\n",
    "final = final.merge(early_funding_features_df, on='org_uuid', how='left')\n",
    "final = final.merge(burn_rate_df, on='org_uuid', how='left')  # burn rate is independent\n",
    "\n",
    "# geography\n",
    "final = final.merge(startup_hub_df, on='org_uuid', how='left')\n",
    "final = final.merge(cohort_density_df, on='org_uuid', how='left')\n",
    "\n",
    "# investor\n",
    "final = final.merge(investor_count_df, on='org_uuid', how='left')\n",
    "final = final.merge(known_investor_df, on='org_uuid', how='left')\n",
    "\n",
    "# outcome\n",
    "final = final.merge(success_df, on='org_uuid', how='left')\n",
    "\n",
    "# clean up final types and missingness\n",
    "cols_to_drop = [col for col in final.columns if col in ['matched_industries', 'early_rounds_2yr', 'num_early_rounds', 'Unnamed: 0']]\n",
    "final = final.drop(columns=cols_to_drop, errors='ignore')\n",
    "final = final.drop_duplicates(subset='org_uuid') # defensive\n",
    "final = final.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_final_data_path = os.path.join(OUTPUT_CB_FOLDER, \"cb_final_data.csv\")\n",
    "final.to_csv(cb_final_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "final = pd.read_csv(cb_final_data_path)\n",
    "print(f\"final shape: {final.shape}\")\n",
    "print(f\"confirming one to one mapping: {final['org_uuid'].is_unique}\")\n",
    "print()\n",
    "print(final.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the industries per startup\n",
    "final['industry_list'] = final['industry'].str.split(',')\n",
    "\n",
    "# Step 2: Explode the list into multiple rows\n",
    "industry_exploded = final.explode('industry_list')\n",
    "\n",
    "# Step 3: Group by industry and count\n",
    "industry_counts = industry_exploded.groupby('industry_list').size().reset_index(name='startup_count')\n",
    "\n",
    "# Step 4: Sort by number of startups (optional, for easier reading)\n",
    "industry_counts = industry_counts.sort_values('startup_count', ascending=False)\n",
    "\n",
    "# Display\n",
    "print(industry_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample rows\")\n",
    "print(final.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = final[final['org_name'].str.lower() == 'zello']\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_label = 'success_label_5y'\n",
    "true_count = (final[success_label] == True).sum()\n",
    "print(true_count)\n",
    "\n",
    "false_count = (final[success_label] == False).sum()\n",
    "print(false_count)\n",
    "\n",
    "# success_label = 'success_label_10y'\n",
    "# true_count = (final[success_label] == True).sum()\n",
    "# print(true_count)\n",
    "\n",
    "# false_count = (final[success_label] == False).sum()\n",
    "# print(false_count)\n",
    "\n",
    "majority_guess = (false_count / (true_count + false_count)) * 100\n",
    "print(f\"majority guess: {majority_guess:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final['outcome_type'].unique())\n",
    "print(final['outcome_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final[final[success_label] == True]\n",
    "temp.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feat Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean\n",
    "\"\"\"\n",
    "# ----------- Load and Preprocess Full Dataset -----------\n",
    "# Full raw dataframe (including macro + ETF rows)\n",
    "econ_df_full = econ_df.copy()\n",
    "\n",
    "# Convert to datetime and sort\n",
    "econ_df_full['date'] = pd.to_datetime(econ_df_full['date'], errors='coerce')\n",
    "econ_df_full = econ_df_full.sort_values('date')\n",
    "\n",
    "# Force all adj_close_* columns to numeric\n",
    "adj_close_cols = [col for col in econ_df_full.columns if col.startswith('adj_close_')]\n",
    "econ_df_full[adj_close_cols] = econ_df_full[adj_close_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# ----------- Monthly Macro Dataset -----------\n",
    "# Drop ETF column and resample all other columns monthly\n",
    "econ_df_macro = (\n",
    "    econ_df_full\n",
    "    .drop(columns=['etf'])  # keep only numeric macro fields\n",
    "    .set_index('date')\n",
    "    .resample('MS')\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ----------- Monthly ETF Dataset -----------\n",
    "# Group by ETF and resample all numeric columns monthly\n",
    "econ_df_etf = (\n",
    "    econ_df_full\n",
    "    .drop(columns=['adj_close_^gspc'])  # avoid duplicate SP500 col\n",
    "    .set_index('date')\n",
    "    .groupby('etf')\n",
    "    .resample('MS')\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(econ_df_macro.dtypes)\n",
    "\n",
    "# Get unique ETF list\n",
    "etf_list = econ_df['etf'].dropna().unique()[2:]\n",
    "print(f\"Unique ETFs: {etf_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_economic_features(econ_df, date):\n",
    "    \"\"\"\n",
    "    Return macroeconomic features for the given date ([-12m, +3m] window).\n",
    "    \"\"\"\n",
    "    date = pd.Timestamp(date)  # Ensure correct datetime type\n",
    "    window = econ_df[(econ_df['date'] >= date - pd.DateOffset(months=12)) &\n",
    "                    (econ_df['date'] <= date + pd.DateOffset(months=3))]\n",
    "\n",
    "    features = {}\n",
    "    features['date'] = date\n",
    "\n",
    "    macro_features = {\n",
    "        'gdp_growth': ['avg', 'delta'],\n",
    "        'interest_rate_fed_funds': ['avg', 'delta'],\n",
    "        'yield_curve_10y_2y': ['avg'],\n",
    "        'unemployment_rate': ['avg'],\n",
    "        'cpi_inflation': ['avg'],\n",
    "        'consumer_sentiment': ['avg', 'z_score'],\n",
    "        'vix_index': ['max'],\n",
    "    }\n",
    "\n",
    "    for col, ops in macro_features.items():\n",
    "        col_series = window[col].dropna()\n",
    "        if col_series.empty:\n",
    "            continue\n",
    "        if 'avg' in ops:\n",
    "            features[f'{col}_avg_15m'] = col_series.mean()\n",
    "        if 'delta' in ops and len(col_series) >= 4:\n",
    "            features[f'{col}_delta_3m'] = col_series.iloc[-1] - col_series.iloc[-4]\n",
    "        if 'z_score' in ops and col_series.std() > 0:\n",
    "            features[f'{col}_z_latest'] = (col_series.iloc[-1] - col_series.mean()) / col_series.std()\n",
    "        if 'max' in ops:\n",
    "            features[f'{col}_max_15m'] = col_series.max()\n",
    "\n",
    "    # binary flags\n",
    "    features['yield_curve_inversion_flag'] = int((window['yield_curve_10y_2y'] < 0).any())\n",
    "    features['vix_spike_flag'] = int((window['vix_index'] > 25).any())\n",
    "\n",
    "    # SP500 as macro momentum indicator using ^GSPC\n",
    "    gspc = window['adj_close_^gspc'].dropna()\n",
    "    if len(gspc) >= 3:\n",
    "        features['sp500_price_change_3m'] = gspc.iloc[-1] - gspc.iloc[-3]\n",
    "        features['sp500_volatility_3m'] = gspc.pct_change().dropna().std()\n",
    "        features['sp500_momentum_latest'] = gspc.iloc[-1] - gspc.mean()\n",
    "    \n",
    "    # Absolute level indicators\n",
    "    if 'vix_index' in window.columns:\n",
    "        features['vix_latest'] = window['vix_index'].dropna().iloc[-1] if not window['vix_index'].dropna().empty else np.nan\n",
    "    if 'interest_rate_fed_funds' in window.columns:\n",
    "        features['fed_funds_rate_latest'] = window['interest_rate_fed_funds'].dropna().iloc[-1] if not window['interest_rate_fed_funds'].dropna().empty else np.nan\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_etf_features(econ_df, date, etf_symbol):\n",
    "    \"\"\"\n",
    "    Return ETF features for a given date and ETF symbol ([-12m, +3m] window).\n",
    "    \"\"\"\n",
    "    date = pd.Timestamp(date)\n",
    "    df = econ_df[(econ_df['etf'] == etf_symbol) &\n",
    "                (econ_df['date'] >= date - pd.DateOffset(months=12)) &\n",
    "                (econ_df['date'] <= date + pd.DateOffset(months=3))]\n",
    "\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')  # enforce numeric\n",
    "    feats = {}\n",
    "    feats[f'{etf_symbol.lower()}_return_3m'] = df['daily_return'].mean()\n",
    "    feats[f'{etf_symbol.lower()}_volatility_3m'] = df['volatility'].mean()\n",
    "    feats[f'{etf_symbol.lower()}_momentum_latest'] = df['momentum'].dropna().iloc[-1] if not df['momentum'].isna().all() else np.nan\n",
    "\n",
    "    if '50-day_ma' in df.columns and '200-day_ma' in df.columns:\n",
    "        if not df[['50-day_ma', '200-day_ma']].isna().all().all():\n",
    "            ma_50 = df['50-day_ma'].iloc[-1]\n",
    "            ma_200 = df['200-day_ma'].iloc[-1]\n",
    "            feats[f'{etf_symbol.lower()}_golden_cross_flag'] = int(ma_50 > ma_200)\n",
    "\n",
    "        price_col = f'adj_close_{etf_symbol.lower()}'\n",
    "        if price_col in df.columns:\n",
    "            price_series = df[price_col].dropna()\n",
    "            if not price_series.empty and not np.isnan(ma_50):\n",
    "                price = price_series.iloc[-1]\n",
    "                feats[f'{etf_symbol.lower()}_ma50_to_price_ratio'] = ma_50 / price if price != 0 else np.nan\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Generate Features -----------\n",
    "\n",
    "# Macro features (1 row per month)\n",
    "macro_feature_rows = []\n",
    "for raw_date in tqdm(econ_df_macro['date'].dropna().unique(), desc=\"Macro\"):\n",
    "    feats = engineer_economic_features(econ_df_macro, raw_date)\n",
    "    macro_feature_rows.append(feats)\n",
    "macro_feat_df = pd.DataFrame(macro_feature_rows)\n",
    "\n",
    "# ETF features (1 row per month, wide format)\n",
    "etf_feature_rows = []\n",
    "for raw_date in tqdm(econ_df_etf['date'].dropna().unique(), desc=\"ETF\"):\n",
    "    date_feats = {'date': raw_date}\n",
    "    for etf in etf_list:\n",
    "        etf_feats = engineer_etf_features(econ_df_etf, raw_date, etf)\n",
    "        date_feats.update(etf_feats)\n",
    "    etf_feature_rows.append(date_feats)\n",
    "etf_feat_df = pd.DataFrame(etf_feature_rows)\n",
    "\n",
    "# Join macro and ETF features on 'date'\n",
    "econ_features_df = pd.merge(macro_feat_df, etf_feat_df, on='date', how='outer').sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_features_df.info()\n",
    "pprint.pprint(macro_feat_df.head(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_out_path = os.path.join(OUTPUT_ECON_FOLDER, \"macro_feature_engineer.csv\")\n",
    "# etf_out_path = os.path.join(OUTPUT_ECON_FOLDER, \"etf_feature_engineer.csv\")\n",
    "# macro_feat_df.to_csv(macro_out_path, index=False)\n",
    "# etf_feat_df.to_csv(etf_out_path, index=False)\n",
    "# print(f\"Saved macro features to: {macro_out_path}\")\n",
    "# print(f\"Saved ETF features to: {etf_out_path}\")\n",
    "\n",
    "econ_feat_out_path = os.path.join(OUTPUT_ECON_FOLDER, \"economic_feature_engineer.csv\")\n",
    "econ_features_df.to_csv(econ_feat_out_path, index=False)\n",
    "print(f\"Saved macro features to: {econ_feat_out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_macro_window(econ_features_df, anchor_date, cols_to_plot):\n",
    "    \"\"\"\n",
    "    Plot a 15-month macroeconomic window centered around anchor_date.\n",
    "    \"\"\"\n",
    "    anchor_date = pd.Timestamp(anchor_date)\n",
    "    window_start = anchor_date - pd.DateOffset(months=12)\n",
    "    window_end = anchor_date + pd.DateOffset(months=3)\n",
    "\n",
    "    window = econ_features_df[(econ_features_df['date'] >= window_start) & (econ_features_df['date'] <= window_end)]\n",
    "\n",
    "    if window.empty:\n",
    "        print(f\"No data found for window around {anchor_date.date()}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, len(cols_to_plot) * 3))\n",
    "    for i, col in enumerate(cols_to_plot, 1):\n",
    "        plt.subplot(len(cols_to_plot), 1, i)\n",
    "        plt.plot(window['date'], window[col], marker='o')\n",
    "        plt.axvline(anchor_date, color='red', linestyle='--', label='Anchor Date')\n",
    "        plt.title(f\"{col} | {window_start.date()} to {window_end.date()}\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(col)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['gdp_growth_avg_15m', 'consumer_sentiment_avg_15m', 'sp500_price_change_3m']\n",
    "plot_macro_window(econ_features_df, anchor_date='2009-03-01', cols_to_plot=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured\n",
    "- Run on Google Colab for GPUs for LLMs on \n",
    "    - Startup Descr + Outlook\n",
    "    - Founder Descr\n",
    "- nlp_processing.ipyb for BERT classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_final_df_path = os.path.join(OUTPUT_CB_FOLDER, \"cb_final_data.csv\")\n",
    "econ_final_df_path = os.path.join(OUTPUT_ECON_FOLDER, \"economic_feature_engineer.csv\")\n",
    "nlp_sentiment_startup_df_path = os.path.join(OUTPUT_NLP_FOLDER, \"nlp_sentiment_features.csv\")\n",
    "nlp_sentiment_outlook_df_path = os.path.join(OUTPUT_NLP_FOLDER, \"nlp_outlook_checkpoint.csv\")\n",
    "nlp_outlook_llm_df_path = os.path.join(OUTPUT_NLP_FOLDER, \"alignment_scores_clean.csv\")\n",
    "nlp_founder_llm_df_path = os.path.join(OUTPUT_NLP_FOLDER, \"founder_strength_scores_clean.csv\")\n",
    "output_merged_df_path = os.path.join(OUTPUT_FINAL_FOLDER, 'merged_startup_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_final_df = pd.read_csv(cb_final_df_path)\n",
    "econ_final_df = pd.read_csv(econ_final_df_path)\n",
    "nlp_startup_sentiment_df = pd.read_csv(nlp_sentiment_startup_df_path)\n",
    "nlp_outlook_sentiment_df = pd.read_csv(nlp_sentiment_outlook_df_path)\n",
    "nlp_outlook_llm_df = pd.read_csv(nlp_outlook_llm_df_path)\n",
    "nlp_founder_llm_df = pd.read_csv(nlp_founder_llm_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Crunchbase Final Data:\\n\")\n",
    "print(cb_final_df.info())\n",
    "print(cb_final_df.head(2))\n",
    "print()\n",
    "print(\"Economic Final Data:\\n\")\n",
    "print(econ_final_df.info())\n",
    "print(econ_final_df.head(2))\n",
    "print()\n",
    "print(\"NLP Startup Sentiment Data:\\n\")\n",
    "print(nlp_startup_sentiment_df.info())\n",
    "print(nlp_startup_sentiment_df.head(2))\n",
    "print()\n",
    "print(\"NLP Outlook Sentiment Data:\\n\")\n",
    "print(nlp_outlook_sentiment_df.info())\n",
    "print(nlp_outlook_sentiment_df.head(2))\n",
    "print()\n",
    "print(\"NLP Outlook LLM Data:\\n\")\n",
    "print(nlp_outlook_llm_df.info())\n",
    "print(nlp_outlook_llm_df.head(2))\n",
    "print()\n",
    "print(\"NLP Founder LLM Data:\\n\")\n",
    "print(nlp_founder_llm_df.info())\n",
    "print(nlp_founder_llm_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_FEATURES_TO_KEEP = [\n",
    "    # Identifiers & Merge Keys\n",
    "    'org_uuid', 'founded_on', 'founded_year', 'industry',\n",
    "    # Target Variables\n",
    "    'success_label_5y', 'success_label_10y',\n",
    "    # Analysis Variables\n",
    "    'org_name', 'outcome_type', 'city', # Keep city for context/viz\n",
    "    # Features\n",
    "    'founder_count', 'founder_gender_diversity', 'has_top_school_founder',\n",
    "    'num_optimal_degrees', 'optimal_degree_ratio', 'is_repeat_founder',\n",
    "    'founder_gender_missing', 'founder_degree_missing', 'founder_school_missing',\n",
    "    'founder_desc_missing', 'age_at_first_funding', 'first_funding_delay',\n",
    "    'early_series_count', 'avg_time_to_early_round_months', 'avg_time_between_rounds',\n",
    "    'burn_rate', 'has_funding_data','num_disclosed_rounds', 'has_disclosed_funding', \n",
    "    'funding_velocity','first_funding_amount_bucket', 'is_startup_hub', \n",
    "    'cohort_funding_density', 'investor_count', 'has_known_investor'\n",
    "    # Raw text descriptions and some derived columns are dropped as planned\n",
    "]\n",
    "\n",
    "BUCKET_TO_ETF = { # Lowercase keys\n",
    "    \"technology\": \"xlk\", \"life sciences\": \"vht\", \"cleantech\": \"pbw\",\n",
    "    \"consumer goods\": \"xly\", \"transportation\": \"iyt\", \"media entertainment and gaming\": \"vox\",\n",
    "    \"telecom\": \"iyz\", \"real estate\": \"vnq\", \"fintech\": \"finx\"\n",
    "}\n",
    "ETF_METRIC_SUFFIXES = [\n",
    "    '_return_3m', '_volatility_3m', '_momentum_latest',\n",
    "    '_golden_cross_flag', '_ma50_to_price_ratio'\n",
    "]\n",
    "\n",
    "# *** DEFINE COLUMNS TO KEEP FROM PRE-CALCULATED ECONOMIC DATA ***\n",
    "GENERAL_ECON_FEATURES_TO_KEEP = [\n",
    "    'gdp_growth_avg_15m', 'gdp_growth_delta_3m',\n",
    "    'interest_rate_fed_funds_avg_15m', 'interest_rate_fed_funds_delta_3m', 'fed_funds_rate_latest',\n",
    "    'yield_curve_10y_2y_avg_15m', 'yield_curve_inversion_flag',\n",
    "    'unemployment_rate_avg_15m',\n",
    "    'cpi_inflation_avg_15m',\n",
    "    'consumer_sentiment_avg_15m', 'consumer_sentiment_z_latest',\n",
    "    'vix_index_max_15m', 'vix_spike_flag', 'vix_latest',\n",
    "    'sp500_price_change_3m', 'sp500_volatility_3m', 'sp500_momentum_latest',\n",
    "]\n",
    "INDIVIDUAL_ETF_FEATURES_TO_KEEP = []\n",
    "etf_tickers = list(set(BUCKET_TO_ETF.values())) # Get unique lowercase tickers\n",
    "for ticker in etf_tickers:\n",
    "    for suffix in ETF_METRIC_SUFFIXES:\n",
    "        INDIVIDUAL_ETF_FEATURES_TO_KEEP.append(f\"{ticker}{suffix}\")\n",
    "\n",
    "ALL_ECON_COLS_TO_KEEP = ['date'] + GENERAL_ECON_FEATURES_TO_KEEP + INDIVIDUAL_ETF_FEATURES_TO_KEEP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Data Preparation Functions ---\n",
    "\n",
    "def prepare_crunchbase(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the Crunchbase dataframe:\n",
    "    - Selects columns based on CB_FEATURES_TO_KEEP.\n",
    "    - Converts 'founded_on' to datetime.\n",
    "    - Imputes missing/invalid 'founded_on' using start of 'founded_year'.\n",
    "    - Creates 'merge_date' column for joining with economic data.\n",
    "    - Drops rows if a merge date cannot be determined.\n",
    "    - Sorts by 'merge_date'.\n",
    "\n",
    "    Args:\n",
    "        df: The raw Crunchbase dataframe.\n",
    "\n",
    "    Returns:\n",
    "        A prepared and sorted dataframe ready for merging.\n",
    "    \"\"\"\n",
    "    print(\"Preparing Crunchbase data...\")\n",
    "    # Assume CB_FEATURES_TO_KEEP is defined globally/passed as argument\n",
    "    cols_to_keep = CB_FEATURES_TO_KEEP\n",
    "    cols_to_keep = [col for col in cols_to_keep if col in df.columns]\n",
    "    prepared_df = df[cols_to_keep].copy()\n",
    "    initial_rows = len(prepared_df)\n",
    "    print(f\"Initial CB rows: {initial_rows}\")\n",
    "\n",
    "    # Attempt to convert founded_on to datetime\n",
    "    prepared_df['founded_on_dt'] = pd.to_datetime(prepared_df['founded_on'], errors='coerce')\n",
    "\n",
    "    # Ensure founded_year is numeric (needed for imputation)\n",
    "    prepared_df['founded_year_num'] = pd.to_numeric(prepared_df['founded_year'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Identify rows where founded_on_dt is NaT (missing or invalid)\n",
    "    missing_date_mask = prepared_df['founded_on_dt'].isna()\n",
    "    num_missing_dates = missing_date_mask.sum()\n",
    "\n",
    "    if num_missing_dates > 0:\n",
    "        print(f\"Found {num_missing_dates} rows with missing/invalid 'founded_on'. Imputing using start of 'founded_year'.\")\n",
    "        # Only impute where year is valid\n",
    "        can_impute_mask = missing_date_mask & prepared_df['founded_year_num'].notna()\n",
    "        num_can_impute = can_impute_mask.sum()\n",
    "        # Create the imputed date string 'YYYY-01-01'\n",
    "        imputed_dates = pd.to_datetime(prepared_df.loc[can_impute_mask, 'founded_year_num'].astype(str) + '-01-01',\n",
    "                                    format='%Y-%m-%d', errors='coerce')\n",
    "        # Fill NaT values in founded_on_dt with the imputed dates\n",
    "        prepared_df.loc[can_impute_mask, 'founded_on_dt'] = imputed_dates\n",
    "        num_failed_impute = num_missing_dates - num_can_impute\n",
    "        if num_failed_impute > 0:\n",
    "            print(f\"Warning: Could not impute date for {num_failed_impute} rows due to missing/invalid 'founded_year'.\")\n",
    "\n",
    "    # Drop rows where founded_on_dt is STILL NaT after imputation (date is unrecoverable)\n",
    "    final_rows_before_drop = len(prepared_df)\n",
    "    prepared_df = prepared_df.dropna(subset=['founded_on_dt'])\n",
    "    final_rows_after_drop = len(prepared_df)\n",
    "    if final_rows_after_drop < final_rows_before_drop:\n",
    "        rows_dropped = final_rows_before_drop - final_rows_after_drop\n",
    "        print(f\"Dropped {rows_dropped} rows where date could not be determined (missing/invalid founded_on AND founded_year).\")\n",
    "\n",
    "    # CRITICAL: Sort by the final, imputed founded_on_dt for merge_asof\n",
    "    prepared_df = prepared_df.sort_values(by='founded_on_dt').reset_index(drop=True)\n",
    "\n",
    "    # Clean up helper columns and rename date column for clarity\n",
    "    prepared_df = prepared_df.drop(columns=['founded_year_num'])\n",
    "    prepared_df = prepared_df.rename(columns={'founded_on_dt': 'merge_date'})\n",
    "\n",
    "    print(f\"Crunchbase data prepared, imputed, and sorted. Final Shape: {prepared_df.shape}\")\n",
    "    return prepared_df\n",
    "\n",
    "def prepare_economic_data_precalculated(df: pd.DataFrame, columns_to_keep: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares pre-calculated economic data: selects specific columns,\n",
    "    converts date, sorts by date.\n",
    "\n",
    "    Args:\n",
    "        df: The raw economic dataframe with pre-calculated features.\n",
    "        columns_to_keep: List of column names to select (must include 'date').\n",
    "\n",
    "    Returns:\n",
    "        A prepared dataframe with selected columns, sorted by date.\n",
    "    \"\"\"\n",
    "    print(\"Preparing pre-calculated Economic data...\")\n",
    "\n",
    "    # Ensure all requested columns exist in the input dataframe\n",
    "    actual_cols_to_keep = [col for col in columns_to_keep if col in df.columns]\n",
    "    missing_cols = set(columns_to_keep) - set(actual_cols_to_keep)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following expected economic columns were not found and will be skipped: {missing_cols}\")\n",
    "    if 'date' not in actual_cols_to_keep :\n",
    "        raise ValueError(\"Critical Error: 'date' column is missing from the economic data frame.\")\n",
    "\n",
    "    econ_prepared = df[actual_cols_to_keep].copy() # Select only specified columns\n",
    "\n",
    "    econ_prepared['date'] = pd.to_datetime(econ_prepared['date'], errors='coerce')\n",
    "    econ_prepared = econ_prepared.dropna(subset=['date']) # Drop rows if date conversion failed\n",
    "    econ_prepared = econ_prepared.sort_values(by='date').reset_index(drop=True) # Sort by date\n",
    "    # Handle potential duplicate dates if necessary\n",
    "    if econ_prepared.duplicated(subset=['date']).any():\n",
    "        print(\"Warning: Duplicate dates found in economic data. Keeping first occurrence.\")\n",
    "        econ_prepared = econ_prepared.drop_duplicates(subset=['date'], keep='first')\n",
    "\n",
    "    print(f\"Pre-calculated Economic data prepared, selected, and sorted. Shape: {econ_prepared.shape}\")\n",
    "    return econ_prepared\n",
    "\n",
    "def engineer_averaged_etf_features(row: pd.Series, bucket_to_etf_map: Dict[str, str], metric_suffixes: List[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates average ETF metrics for a startup based on its industry\n",
    "    using the pre-calculated ETF columns already merged onto the row.\n",
    "\n",
    "    Args:\n",
    "        row: A row from the merged dataframe (after merge_asof).\n",
    "        bucket_to_etf_map: Dictionary mapping industry buckets (lowercase) to ETF tickers (lowercase).\n",
    "        metric_suffixes: List of suffixes for ETF metrics (e.g., '_return_3m').\n",
    "\n",
    "    Returns:\n",
    "        A Pandas Series containing the averaged ETF metrics for the startup.\n",
    "    \"\"\"\n",
    "    industry_str = row.get('industry')\n",
    "    averaged_features = {}\n",
    "\n",
    "    # Initialize features with NaN\n",
    "    for suffix in metric_suffixes: averaged_features[f'avg_etf{suffix}'] = np.nan\n",
    "\n",
    "    if pd.isna(industry_str) or not isinstance(industry_str, str):\n",
    "        return pd.Series(averaged_features, dtype='float64')\n",
    "\n",
    "    # Parse industries (lowercase) and find corresponding ETFs (lowercase)\n",
    "    industries = [ind.strip().lower() for ind in industry_str.split(',')]\n",
    "    relevant_etfs = list(set(bucket_to_etf_map.get(ind) for ind in industries if ind in bucket_to_etf_map)) # Use .get for safety\n",
    "\n",
    "    if not relevant_etfs:\n",
    "        return pd.Series(averaged_features, dtype='float64')\n",
    "\n",
    "    # Calculate average for each metric type across relevant ETFs\n",
    "    for suffix in metric_suffixes:\n",
    "        metric_values = []\n",
    "        for etf_ticker in relevant_etfs:\n",
    "            col_name = f\"{etf_ticker}{suffix}\" # Construct column name (e.g., xlk_return_3m)\n",
    "            # Check if column exists in the row and if the value is not NaN\n",
    "            if col_name in row and pd.notna(row[col_name]):\n",
    "                metric_values.append(row[col_name])\n",
    "        # Calculate average if we collected any values\n",
    "        if metric_values:\n",
    "            averaged_features[f'avg_etf{suffix}'] = np.mean(metric_values)\n",
    "        # else: feature remains NaN (already initialized)\n",
    "\n",
    "    return pd.Series(averaged_features, dtype='float64')\n",
    "\n",
    "def prepare_nlp_startup_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the NLP startup/founder sentiment data by selecting\n",
    "    FinBERT signed scores and similarity scores, then renaming columns.\n",
    "\n",
    "    Args:\n",
    "        df: The raw NLP startup/founder sentiment dataframe.\n",
    "\n",
    "    Returns:\n",
    "        A prepared dataframe with selected and renamed columns.\n",
    "    \"\"\"\n",
    "    print(\"Preparing NLP Startup/Founder Sentiment data...\")\n",
    "    cols_to_keep = [\n",
    "        'org_uuid',\n",
    "        'org_finbert_numeric_signed',\n",
    "        'founder_sentiment_numeric_signed',\n",
    "        'org_sim_to_exemplar',\n",
    "        'founder_sim_to_exemplar'\n",
    "    ]\n",
    "    # Ensure all selected columns exist\n",
    "    cols_to_keep = [col for col in cols_to_keep if col in df.columns]\n",
    "    prepared_df = df[cols_to_keep].copy()\n",
    "\n",
    "    rename_dict = {\n",
    "        'org_finbert_numeric_signed': 'org_desc_sentiment_finbert',\n",
    "        'founder_sentiment_numeric_signed': 'founder_desc_sentiment_finbert',\n",
    "        'org_sim_to_exemplar': 'org_desc_sim_exemplar',\n",
    "        'founder_sim_to_exemplar': 'founder_desc_sim_exemplar'\n",
    "    }\n",
    "    prepared_df = prepared_df.rename(columns=rename_dict)\n",
    "    print(f\"NLP Startup/Founder Sentiment data prepared. Shape: {prepared_df.shape}\")\n",
    "    return prepared_df\n",
    "\n",
    "def prepare_llm_founder_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the LLM founder score data. Assumes pre-processing handles\n",
    "    duplicates, otherwise add .drop_duplicates(subset=['org_uuid'], keep='first') here.\n",
    "\n",
    "    Args:\n",
    "        df: The raw LLM founder score dataframe.\n",
    "\n",
    "    Returns:\n",
    "        A prepared dataframe with selected and renamed columns.\n",
    "    \"\"\"\n",
    "    print(\"Preparing LLM Founder Score data...\")\n",
    "    # Note: Assuming the input df is already pre-processed to have one score per org_uuid.\n",
    "    # If not, uncomment the following line:\n",
    "    # df = df.drop_duplicates(subset=['org_uuid'], keep='first')\n",
    "\n",
    "    cols_to_keep = ['org_uuid', 'score']\n",
    "    # Ensure all selected columns exist\n",
    "    cols_to_keep = [col for col in cols_to_keep if col in df.columns]\n",
    "    prepared_df = df[cols_to_keep].copy()\n",
    "\n",
    "    prepared_df = prepared_df.rename(columns={'score': 'llm_founder_score'})\n",
    "\n",
    "    # Handle potential NaN scores if needed (e.g., fill with median or specific value)\n",
    "    # Example: prepared_df['llm_founder_score'] = prepared_df['llm_founder_score'].fillna(prepared_df['llm_founder_score'].median())\n",
    "\n",
    "    print(f\"LLM Founder Score data prepared. Shape: {prepared_df.shape}\")\n",
    "    return prepared_df\n",
    "\n",
    "def prepare_llm_outlook_alignment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the LLM outlook alignment data by aggregating scores per org_uuid,\n",
    "    calculating the mean, and creating a binned version.\n",
    "\n",
    "    Args:\n",
    "        df: The raw LLM outlook alignment dataframe.\n",
    "\n",
    "    Returns:\n",
    "        A prepared dataframe with aggregated and binned scores.\n",
    "    \"\"\"\n",
    "    print(\"Preparing LLM Outlook Alignment data...\")\n",
    "    # Ensure score is numeric, coerce errors to NaN\n",
    "    df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "    # Drop rows where score could not be parsed or is missing before aggregation\n",
    "    df = df.dropna(subset=['score'])\n",
    "\n",
    "    # Aggregate: Calculate mean score per org_uuid\n",
    "    agg_df = df.groupby('org_uuid')['score'].mean().reset_index()\n",
    "    agg_df = agg_df.rename(columns={'score': 'llm_outlook_align_score_avg'})\n",
    "\n",
    "        # Binning: Create binned score (-1, 0, 1)\n",
    "    # Adjusted conditions to cover averages between 4 and 5\n",
    "    conditions = [\n",
    "        agg_df['llm_outlook_align_score_avg'] < 3.5,         # Scores 1, 2, 3 and averages < 4\n",
    "        (agg_df['llm_outlook_align_score_avg'] >= 3.5) & (agg_df['llm_outlook_align_score_avg'] < 4.5),         # Score 4 and averages >= 4 but < 5\n",
    "        agg_df['llm_outlook_align_score_avg'] >= 4.5         # Score 5 and averages == 5\n",
    "    ]\n",
    "    choices = [-1, 0, 1]\n",
    "    agg_df['llm_outlook_align_score_binned'] = np.select(conditions, choices, default=np.nan) # Default NaN handles any unexpected cases or NaNs in avg\n",
    "\n",
    "    print(f\"LLM Outlook Alignment data prepared. Shape: {agg_df.shape}\")\n",
    "    return agg_df[['org_uuid', 'llm_outlook_align_score_avg', 'llm_outlook_align_score_binned']]\n",
    "\n",
    "def prepare_nlp_outlook_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the NLP outlook sentiment data by calculating FinBERT net scores\n",
    "    for outlook and timing signal sections, selecting relevant columns, and renaming.\n",
    "\n",
    "    Args:\n",
    "        df: The raw NLP outlook sentiment dataframe.\n",
    "\n",
    "    Returns:\n",
    "        A prepared dataframe with industry/year FinBERT net sentiment scores.\n",
    "    \"\"\"\n",
    "    print(\"Preparing NLP Outlook Sentiment data...\")\n",
    "\n",
    "    # Calculate FinBERT net sentiment scores\n",
    "    # Ensure necessary columns exist before calculation\n",
    "    if 'outlook_finbert_positive' in df.columns and 'outlook_finbert_negative' in df.columns:\n",
    "        df['industry_outlook_sentiment_finbert'] = df['outlook_finbert_positive'] - df['outlook_finbert_negative']\n",
    "    else:\n",
    "        print(\"Warning: Outlook FinBERT columns not found. Setting sentiment to NaN.\")\n",
    "        df['industry_outlook_sentiment_finbert'] = np.nan\n",
    "\n",
    "    if 'timing_signal_finbert_positive' in df.columns and 'timing_signal_finbert_negative' in df.columns:\n",
    "        df['industry_timing_sentiment_finbert'] = df['timing_signal_finbert_positive'] - df['timing_signal_finbert_negative']\n",
    "    else:\n",
    "        print(\"Warning: Timing Signal FinBERT columns not found. Setting sentiment to NaN.\")\n",
    "        df['industry_timing_sentiment_finbert'] = np.nan\n",
    "\n",
    "    cols_to_keep = ['industry', 'year', 'industry_outlook_sentiment_finbert', 'industry_timing_sentiment_finbert']\n",
    "    # Filter out columns that weren't created if source columns were missing\n",
    "    cols_to_keep = [col for col in cols_to_keep if col in df.columns]\n",
    "\n",
    "    prepared_df = df[cols_to_keep].copy()\n",
    "\n",
    "    # Normalize industry names for merging (lowercase, strip whitespace)\n",
    "    if 'industry' in prepared_df.columns:\n",
    "        prepared_df['industry'] = prepared_df['industry'].str.lower().str.strip()\n",
    "    else:\n",
    "        print(\"Warning: 'industry' column not found in NLP Outlook Sentiment data.\")\n",
    "        # Handle appropriately - maybe return empty df or raise error depending on strictness needed\n",
    "        return pd.DataFrame(columns=cols_to_keep) # Return empty structure\n",
    "\n",
    "    print(f\"NLP Outlook Sentiment data prepared. Shape: {prepared_df.shape}\")\n",
    "    return prepared_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Helper Functions for Merging ---\n",
    "\n",
    "def get_primary_industry(industry_str: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the first industry from a comma-separated string.\n",
    "\n",
    "    Args:\n",
    "        industry_str: Comma-separated string of industries.\n",
    "\n",
    "    Returns:\n",
    "        The first industry found, or None if input is invalid.\n",
    "    \"\"\"\n",
    "    if pd.isna(industry_str) or not isinstance(industry_str, str) or not industry_str.strip():\n",
    "        return None\n",
    "    return industry_str.split(',')[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Merging Logic ---\n",
    "\n",
    "def merge_datasets(cb_df: pd.DataFrame, # Sorted by merge_date\n",
    "                    econ_df: pd.DataFrame, # Sorted by date, pre-calculated & selected features\n",
    "                    nlp_startup_df: pd.DataFrame,\n",
    "                    llm_founder_df: pd.DataFrame,\n",
    "                    llm_outlook_df: pd.DataFrame,\n",
    "                    nlp_outlook_df: pd.DataFrame,\n",
    "                    bucket_to_etf_map: Dict[str, str],\n",
    "                    etf_metric_suffixes: List[str]\n",
    "                    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges prepared dataframes into a single master dataframe.\n",
    "    - Uses pd.merge_asof to join selected pre-calculated economic data.\n",
    "    - Merges NLP/LLM datasets based on org_uuid.\n",
    "    - Merges NLP Outlook data based on founded_year and primary industry.\n",
    "    - Engineers averaged ETF features from merged individual ETF columns.\n",
    "    - Cleans up by dropping individual ETF columns.\n",
    "\n",
    "    Args:\n",
    "        cb_df: Prepared Crunchbase data (sorted by 'merge_date').\n",
    "        econ_df: Prepared Economic data with selected features (sorted by 'date').\n",
    "        nlp_startup_df: Prepared NLP Startup/Founder Sentiment data.\n",
    "        llm_founder_df: Prepared LLM Founder Score data.\n",
    "        llm_outlook_df: Prepared LLM Outlook Alignment data.\n",
    "        nlp_outlook_df: Prepared NLP Outlook Sentiment data.\n",
    "        bucket_to_etf_map: Dictionary mapping industry buckets (lowercase) to ETF tickers (lowercase).\n",
    "        etf_metric_suffixes: List of suffixes for ETF metrics (e.g., '_return_3m').\n",
    "\n",
    "    Returns:\n",
    "        The final merged and cleaned master dataframe.\n",
    "    \"\"\"\n",
    "    print(\"Starting dataset merging...\")\n",
    "\n",
    "    # 1. Merge Economic Data using merge_asof\n",
    "    print(\"Performing merge_asof for economic data...\")\n",
    "    master_df = pd.merge_asof(\n",
    "        cb_df,\n",
    "        econ_df,\n",
    "        left_on='merge_date', # Use the potentially imputed date from CB\n",
    "        right_on='date',      # Use the date from Econ data\n",
    "        direction='backward'  # Use data on or before the merge_date\n",
    "    )\n",
    "    if 'date' in master_df.columns: master_df = master_df.drop(columns=['date']) # Drop redundant date column\n",
    "    print(f\"Shape after merging Economic Data: {master_df.shape}\")\n",
    "    if econ_df.shape[1] > 1 : # Check if econ_df had columns other than 'date'\n",
    "        first_econ_col = econ_df.columns[1];\n",
    "        if first_econ_col in master_df.columns:\n",
    "            failed_merge_count = master_df[first_econ_col].isnull().sum()\n",
    "            if failed_merge_count > 0: print(f\"Info: {failed_merge_count} rows have NaN economic features (check merge_asof results / original data).\")\n",
    "\n",
    "    # 2. Merge NLP/LLM data on org_uuid\n",
    "    master_df = pd.merge(master_df, nlp_startup_df, on='org_uuid', how='left')\n",
    "    master_df = pd.merge(master_df, llm_founder_df, on='org_uuid', how='left')\n",
    "    master_df = pd.merge(master_df, llm_outlook_df, on='org_uuid', how='left')\n",
    "    print(f\"Shape after merging NLP/LLM data: {master_df.shape}\")\n",
    "\n",
    "    # 3. Merge NLP Outlook Sentiment (on founded_year and primary_industry)\n",
    "    master_df['primary_industry'] = master_df['industry'].apply(get_primary_industry)\n",
    "    master_df['primary_industry_normalized'] = master_df['primary_industry'].str.lower().str.strip()\n",
    "    master_df = pd.merge(master_df, nlp_outlook_df,\n",
    "                        left_on=['founded_year', 'primary_industry_normalized'],\n",
    "                        right_on=['year', 'industry'],\n",
    "                        how='left',\n",
    "                        suffixes=('', '_nlp_outlook'))\n",
    "    master_df = master_df.drop(columns=['year', 'industry_nlp_outlook', 'primary_industry', 'primary_industry_normalized'], errors='ignore')\n",
    "    print(f\"Shape after merging NLP Outlook Sentiment: {master_df.shape}\")\n",
    "\n",
    "    # 4. Engineer Averaged ETF Features\n",
    "    print(\"Engineering averaged ETF features...\")\n",
    "    averaged_etf_features = master_df.apply(\n",
    "        engineer_averaged_etf_features,\n",
    "        axis=1,\n",
    "        args=(bucket_to_etf_map, etf_metric_suffixes) # Pass map and suffixes\n",
    "    )\n",
    "    master_df = pd.concat([master_df, averaged_etf_features], axis=1)\n",
    "    print(f\"Shape after adding averaged ETF features: {master_df.shape}\")\n",
    "\n",
    "    # 5. Cleanup: Drop individual ETF columns\n",
    "    print(\"Dropping individual ETF columns...\")\n",
    "    cols_to_drop = []\n",
    "    etf_tickers = list(set(bucket_to_etf_map.values())) # Get unique lowercase tickers\n",
    "    for ticker in etf_tickers:\n",
    "        for suffix in etf_metric_suffixes:\n",
    "            col_name = f\"{ticker}{suffix}\"\n",
    "            if col_name in master_df.columns:\n",
    "                cols_to_drop.append(col_name)\n",
    "    if cols_to_drop:\n",
    "        master_df = master_df.drop(columns=cols_to_drop)\n",
    "        print(f\"Dropped {len(cols_to_drop)} individual ETF columns.\")\n",
    "    else: print(\"No individual ETF columns found to drop.\")\n",
    "\n",
    "    print(\"Merging and ETF feature engineering completed.\")\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Individual DataFrames\n",
    "cb_prepared = prepare_crunchbase(cb_final_df)\n",
    "nlp_startup_prepared = prepare_nlp_startup_sentiment(nlp_startup_sentiment_df)\n",
    "llm_founder_prepared = prepare_llm_founder_score(nlp_founder_llm_df)\n",
    "llm_outlook_prepared = prepare_llm_outlook_alignment(nlp_outlook_llm_df)\n",
    "econ_prepared = prepare_economic_data_precalculated(econ_final_df, ALL_ECON_COLS_TO_KEEP)\n",
    "nlp_outlook_prepared = prepare_nlp_outlook_sentiment(nlp_outlook_sentiment_df)\n",
    "\n",
    "# Merge DataFrames using the updated merge_datasets function\n",
    "master_df = merge_datasets(\n",
    "    cb_prepared,\n",
    "    econ_prepared, # Pass the prepared economic data\n",
    "    nlp_startup_prepared,\n",
    "    llm_founder_prepared,\n",
    "    llm_outlook_prepared,\n",
    "    nlp_outlook_prepared,\n",
    "    BUCKET_TO_ETF, # Pass the mapping\n",
    "    ETF_METRIC_SUFFIXES # Pass the suffixes\n",
    ")\n",
    "\n",
    "# Final Checks and Save\n",
    "print(\"\\n--- Final Merged DataFrame Info ---\")\n",
    "final_expected_rows = len(cb_prepared) # Expected rows after CB prep\n",
    "print(f\"Final Shape: {master_df.shape}\")\n",
    "print(f\"Expected rows based on prepared CB data: {final_expected_rows}\")\n",
    "# Check if the number of rows matches the prepared CB data\n",
    "if master_df.shape[0] != final_expected_rows:\n",
    "    print(f\"Warning: Row count mismatch! Expected {final_expected_rows}, got {master_df.shape[0]}. Check merge steps.\")\n",
    "else:\n",
    "    print(f\"Row count matches expected ({master_df.shape[0]} rows).\")\n",
    "\n",
    "# Display info and missing values\n",
    "print(master_df.info(verbose=True, max_cols=200)) # Show more columns if needed\n",
    "print(\"\\nMissing Values per Column (Showing columns with any NaNs):\")\n",
    "null_counts = master_df.isnull().sum(); print(null_counts[null_counts > 0].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nSample of Final Data (First 5 Rows):\")\n",
    "print(master_df.head())\n",
    "\n",
    "# Save the final dataframe\n",
    "master_df.to_csv(output_merged_df_path, index=False)\n",
    "print(f\"\\nSuccessfully saved the merged dataset to: {output_merged_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final[final['org_name'] ==  'Meta']\n",
    "print(temp['age_at_first_funding'])\n",
    "print(temp['first_funding_delay'])\n",
    "print(temp['early_series_count'])\n",
    "print(temp['avg_time_to_early_round_months'])\n",
    "print(temp['avg_time_between_rounds'])\n",
    "print(temp['industry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.read_csv(output_merged_df_path)\n",
    "print(final_dataset.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
