{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructued Sentiment Data Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# nlp librarys\n",
    "# import nltk\n",
    "# nltk.download('punkt', download_dir='../.venv/nltk_data')\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# run command in terminal: /Applications/Python\\ 3.11/Install\\ Certificates.command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDERS\n",
    "OUTPUT_FOLDER = \"../Data/Output\"\n",
    "INPUT_FOLDER = \"../Data/Input\"\n",
    "\n",
    "INPUT_NLP_FOLDER = os.path.join(INPUT_FOLDER, \"NLP\")\n",
    "OUTPUT_NLP_FOLDER = os.path.join(OUTPUT_FOLDER, \"NLP\")\n",
    "perplexity_output_path = os.path.join(OUTPUT_NLP_FOLDER, \"perplexity.csv\")\n",
    "\n",
    "# Check contents of folders\n",
    "output_contents = os.listdir(OUTPUT_NLP_FOLDER)\n",
    "print(output_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity API\n",
    "https://docs.perplexity.ai/guides/getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETTINGS ===\n",
    "api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "model = \"sonar-pro\"\n",
    "temperature = 0.3\n",
    "resume_file = os.path.exists(perplexity_output_path)\n",
    "\n",
    "years = [\n",
    "    2004, 2005, 2006, 2007, 2008, 2009, 2010,\n",
    "    2011, 2012, 2013, 2014, 2015,\n",
    "    2016, 2017, 2018, 2019, 2020,\n",
    "    2021, 2022, 2023, 2024, 2025\n",
    "]\n",
    "\n",
    "industries = [\n",
    "    'Cleantech', 'Consumer Goods', 'Fintech', 'Life Sciences',\n",
    "    'Media, Entertainment and Gaming', 'Real Estate',\n",
    "    'Technology', 'Telecom', 'Transportation'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROMPT BUILDER ===\n",
    "def build_prompt(industry, year):\n",
    "    return f\"\"\"\n",
    "    You are a business and industry researcher evaluating how the external environment and timing in {year} influenced the viability of launching a startup in the {industry} sector.\n",
    "\n",
    "    Your task is to review reputable sources (such as government forecasts, analyst reports or outlooks, and industry coverage) and provide insight into the state and outlook of the industry in {year}. Focus on macroeconomic, technological, regulatory, and consumer demand trends that would influence whether a new company could succeed in this space.\n",
    "\n",
    "    If there was little momentum or coverage for the industry in {year}, briefly state that. Otherwise, structure your findings into the following labeled sections:\n",
    "\n",
    "    ## Summary  \n",
    "    What was the overall state of the {industry} industry in {year}?\n",
    "\n",
    "    ## Trends  \n",
    "    What key trends were emerging, either in technology, demand, or behavior?\n",
    "\n",
    "    ## Infrastructure  \n",
    "    Was the market supported by enabling technologies, regulations, or platforms (e.g., mobile adoption, cloud computing, data infrastructure, regulatory clarity)?\n",
    "\n",
    "    ## Outlook  \n",
    "    Were analysts or institutions projecting strong growth, stagnation, or uncertainty for the industry?\n",
    "\n",
    "    ## Timing Signal  \n",
    "    Based on the above, would {year} be a promising time to launch a startup in this industry? Summarize with reasoning.\n",
    "    \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PERPLEXITY API CALL ===\n",
    "def query_perplexity(prompt):\n",
    "    url = \"https://api.perplexity.ai/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 0,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION PARSER ===\n",
    "def extract_section(response, tag):\n",
    "    pattern = fr\"##\\s*{tag}\\s*(.*?)\\s*(?=(##|$))\"\n",
    "    match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def parse_response(response):\n",
    "    return {\n",
    "        \"summary\": extract_section(response, \"Summary\"),\n",
    "        \"trends\": extract_section(response, \"Trends\"),\n",
    "        \"infrastructure\": extract_section(response, \"Infrastructure\"),\n",
    "        \"outlook\": extract_section(response, \"Outlook\"),\n",
    "        \"timing_signal\": extract_section(response, \"Timing Signal\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INTERMEDIATE LOGGER ===\n",
    "def log_row_to_csv(row):\n",
    "    file_exists = os.path.exists(perplexity_output_path)\n",
    "    with open(perplexity_output_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN LOOP ===\n",
    "def run_perplexity():\n",
    "    results = []\n",
    "    total_queries = len(industries) * len(years)\n",
    "    token_total = 0\n",
    "\n",
    "    for i, (industry, year) in enumerate([(a, b) for a in industries for b in years]):\n",
    "        prompt = build_prompt(industry, year)\n",
    "        print(f\"[{i+1}/{total_queries}] Querying: {industry} {year}...\")\n",
    "\n",
    "        try:\n",
    "            response = query_perplexity(prompt)\n",
    "            parsed = parse_response(response)\n",
    "\n",
    "            estimated_tokens = int(1.33 * (len(prompt.split()) + len(response.split())))\n",
    "            token_total += estimated_tokens\n",
    "\n",
    "            row = {\n",
    "                \"industry\": industry,\n",
    "                \"year\": year,\n",
    "                **parsed,\n",
    "                \"tokens_estimate\": estimated_tokens\n",
    "            }\n",
    "\n",
    "            results.append(row)\n",
    "            log_row_to_csv(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {industry} {year} â€“ {e}\")\n",
    "            row = {\n",
    "                \"industry\": industry,\n",
    "                \"year\": year,\n",
    "                \"summary\": \"\",\n",
    "                \"trends\": \"\",\n",
    "                \"infrastructure\": \"\",\n",
    "                \"outlook\": \"\",\n",
    "                \"timing_signal\": f\"ERROR: {e}\",\n",
    "                \"tokens_estimate\": 0\n",
    "            }\n",
    "            results.append(row)\n",
    "            log_row_to_csv(row)\n",
    "\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    # === SUMMARY ===\n",
    "    print(f\"\\nComplete! {len(results)} queries processed.\")\n",
    "    print(f\"Estimated total tokens used: {token_total:,}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_results = run_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(perplexity_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
